{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pose CNN (Along with Hyperparameter tunings).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amithrx/Live-Pose-estimation-using-Google-Mediapipe-and-Various-ML-algorithms/blob/main/Pose_CNN_(Along_with_Hyperparameter_tunings).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vz4DxPP1hqFW"
      },
      "outputs": [],
      "source": [
        "# importing libraries\n",
        "%matplotlib inline\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Sequential \n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.constraints import maxnorm\n",
        "import os "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_tuner\n",
        "import keras_tuner as kt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTjONJbWL9Ks",
        "outputId": "adfd9454-6729-4069-fcda-4fa2fc1e66b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras_tuner\n",
            "  Downloading keras_tuner-1.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (2.23.0)\n",
            "Collecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (5.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (21.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (1.21.6)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (2.8.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras_tuner) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->keras_tuner) (57.4.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->keras_tuner) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras_tuner) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras_tuner) (4.4.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras_tuner) (5.1.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras_tuner) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->keras_tuner) (1.0.18)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras_tuner) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras_tuner) (0.2.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras_tuner) (3.0.9)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras_tuner) (0.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras_tuner) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras_tuner) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras_tuner) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras_tuner) (2.10)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (1.47.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (3.3.7)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (1.35.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (0.37.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras_tuner) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras_tuner) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras_tuner) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras_tuner) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras_tuner) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras_tuner) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras_tuner) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras_tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras_tuner) (3.2.0)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.1.2 kt-legacy-1.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#For accessing any file from google drive, first share it for public access. Copy its id from last part of its address. Then specify the two lines below.\n",
        "# downloaded1 = drive.CreateFile({'id':\"13NkbScVe7i0Hegj1Q-1cGOHr0V7CGnua\"})   # replace the id with id of file you want to access\n",
        "downloaded1 = drive.CreateFile({'id':\"1rddzkvF88xQEzMWnGEoR_zbXQejHoAdv\"})   # replace the id with id of file you want to access\n",
        "downloaded1.GetContentFile('finalcut.csv')"
      ],
      "metadata": {
        "id": "uL-pHBd8ijD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reading raw data file\n",
        "#columns = ['x-axis', 'y-axis', 'z-axis', 'timestamp', 'activity']\n",
        "df_har = pd.read_csv(\"finalcut.csv\", header = None)\n",
        "df_har\n"
      ],
      "metadata": {
        "id": "S8NEbv1xhuU3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "d4e6fe07-4be0-418c-a150-6375d7aa75d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               0            1             2            3            4   \\\n",
              "0          nose_x  l_shouldr_x  r_shoulder_x    l_elbow_x    r_elbow_x   \n",
              "1     0.466836989  0.536425471   0.421737492  0.552586377  0.400636822   \n",
              "2     0.466981173  0.534423232   0.419800907  0.552579224  0.398616493   \n",
              "3      0.44387275  0.510528922    0.39275986  0.538784623  0.373896241   \n",
              "4     0.464225858  0.529767096   0.413535655    0.5512923  0.381715655   \n",
              "...           ...          ...           ...          ...          ...   \n",
              "3808  0.410130262  0.470612764   0.353161514  0.481383622  0.346614212   \n",
              "3809  0.413310617  0.471635044   0.354795247  0.481459528   0.34364754   \n",
              "3810  0.425183952  0.478965431   0.361634165  0.487567008  0.350379795   \n",
              "3811  0.424494207  0.479368269   0.360941797  0.486462504  0.352958918   \n",
              "3812  0.415483415  0.472029865   0.358732998  0.484145254  0.349483103   \n",
              "\n",
              "               5            6            7            8            9   ...  \\\n",
              "0       l_wrist_x    r_wrist_x      l_hip_x      r_hip_x     l_knee_x  ...   \n",
              "1     0.554411948  0.395507991  0.511357903  0.443766415  0.519800663  ...   \n",
              "2     0.550638914  0.395376265   0.51000458  0.442397594  0.518164456  ...   \n",
              "3     0.515839875  0.406628519  0.494533837  0.429210663  0.511657953  ...   \n",
              "4     0.495894909  0.386194438  0.497234821  0.429964721  0.494289339  ...   \n",
              "...           ...          ...          ...          ...          ...  ...   \n",
              "3808  0.484839678  0.346404254  0.446524382  0.382068634  0.442739815  ...   \n",
              "3809  0.488360822  0.344700277  0.447434902  0.384895116  0.442575783  ...   \n",
              "3810  0.493800342  0.350503743  0.453244925  0.391087115  0.453847826  ...   \n",
              "3811  0.491775751  0.352376729  0.454283655  0.392247468  0.451816946  ...   \n",
              "3812   0.49168691  0.348622948  0.453927696  0.391181767  0.451674372  ...   \n",
              "\n",
              "                30            31            32            33            34  \\\n",
              "0        r_elbow_z     l_wrist_z     r_wrist_z       l_hip_z       r_hip_z   \n",
              "1     -0.127180368  -0.253910035  -0.229280859   -0.00820767   0.008048343   \n",
              "2     -0.134703189    -0.2186542  -0.265969336  -0.006716055   0.006530192   \n",
              "3     -0.233621523  -0.306831658   -0.44258517  -0.004903356   0.004764312   \n",
              "4      0.010997185  -0.524008512  -0.154198542  -0.020188037   0.020177683   \n",
              "...            ...           ...           ...           ...           ...   \n",
              "3808  -0.047307201   -0.20736447  -0.169882715  -0.009812003   0.009730845   \n",
              "3809   -0.14865613  -0.192228019   -0.25464493  -0.000278628    0.00014017   \n",
              "3810  -0.069936648  -0.114805818  -0.167545915   0.005705269  -0.005720128   \n",
              "3811   -0.05203687  -0.222686455  -0.166772068   -0.00548711   0.005477537   \n",
              "3812  -0.110999048  -0.155995175  -0.221138313   0.005661204  -0.005736118   \n",
              "\n",
              "                35            36           37            38      39  \n",
              "0         l_knee_z      r_knee_z    l_ankle_z     r_ankle_z  Output  \n",
              "1      0.075822279   0.088045202   0.28819102   0.313930452       2  \n",
              "2      0.065338001   0.060155779  0.251547188   0.270694315       2  \n",
              "3      0.022069298   0.015744209  0.255176365   0.251044512       2  \n",
              "4     -0.029583782  -0.306580335  0.362366378  -0.018393395       2  \n",
              "...            ...           ...          ...           ...     ...  \n",
              "3808  -0.016656373   0.036118161  0.248286009   0.303717762       1  \n",
              "3809  -0.016059125  -0.000954395  0.230153367   0.250322253       1  \n",
              "3810  -0.020812653  -0.015095033  0.213164449    0.20433481       1  \n",
              "3811   0.011075634   0.045946468  0.289274305   0.309554756       1  \n",
              "3812   0.010790403   0.009689408  0.284101754   0.279072672       1  \n",
              "\n",
              "[3813 rows x 40 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ce3e218f-2357-49f4-a0bd-2be60977accf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>nose_x</td>\n",
              "      <td>l_shouldr_x</td>\n",
              "      <td>r_shoulder_x</td>\n",
              "      <td>l_elbow_x</td>\n",
              "      <td>r_elbow_x</td>\n",
              "      <td>l_wrist_x</td>\n",
              "      <td>r_wrist_x</td>\n",
              "      <td>l_hip_x</td>\n",
              "      <td>r_hip_x</td>\n",
              "      <td>l_knee_x</td>\n",
              "      <td>...</td>\n",
              "      <td>r_elbow_z</td>\n",
              "      <td>l_wrist_z</td>\n",
              "      <td>r_wrist_z</td>\n",
              "      <td>l_hip_z</td>\n",
              "      <td>r_hip_z</td>\n",
              "      <td>l_knee_z</td>\n",
              "      <td>r_knee_z</td>\n",
              "      <td>l_ankle_z</td>\n",
              "      <td>r_ankle_z</td>\n",
              "      <td>Output</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.466836989</td>\n",
              "      <td>0.536425471</td>\n",
              "      <td>0.421737492</td>\n",
              "      <td>0.552586377</td>\n",
              "      <td>0.400636822</td>\n",
              "      <td>0.554411948</td>\n",
              "      <td>0.395507991</td>\n",
              "      <td>0.511357903</td>\n",
              "      <td>0.443766415</td>\n",
              "      <td>0.519800663</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.127180368</td>\n",
              "      <td>-0.253910035</td>\n",
              "      <td>-0.229280859</td>\n",
              "      <td>-0.00820767</td>\n",
              "      <td>0.008048343</td>\n",
              "      <td>0.075822279</td>\n",
              "      <td>0.088045202</td>\n",
              "      <td>0.28819102</td>\n",
              "      <td>0.313930452</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.466981173</td>\n",
              "      <td>0.534423232</td>\n",
              "      <td>0.419800907</td>\n",
              "      <td>0.552579224</td>\n",
              "      <td>0.398616493</td>\n",
              "      <td>0.550638914</td>\n",
              "      <td>0.395376265</td>\n",
              "      <td>0.51000458</td>\n",
              "      <td>0.442397594</td>\n",
              "      <td>0.518164456</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.134703189</td>\n",
              "      <td>-0.2186542</td>\n",
              "      <td>-0.265969336</td>\n",
              "      <td>-0.006716055</td>\n",
              "      <td>0.006530192</td>\n",
              "      <td>0.065338001</td>\n",
              "      <td>0.060155779</td>\n",
              "      <td>0.251547188</td>\n",
              "      <td>0.270694315</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.44387275</td>\n",
              "      <td>0.510528922</td>\n",
              "      <td>0.39275986</td>\n",
              "      <td>0.538784623</td>\n",
              "      <td>0.373896241</td>\n",
              "      <td>0.515839875</td>\n",
              "      <td>0.406628519</td>\n",
              "      <td>0.494533837</td>\n",
              "      <td>0.429210663</td>\n",
              "      <td>0.511657953</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.233621523</td>\n",
              "      <td>-0.306831658</td>\n",
              "      <td>-0.44258517</td>\n",
              "      <td>-0.004903356</td>\n",
              "      <td>0.004764312</td>\n",
              "      <td>0.022069298</td>\n",
              "      <td>0.015744209</td>\n",
              "      <td>0.255176365</td>\n",
              "      <td>0.251044512</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.464225858</td>\n",
              "      <td>0.529767096</td>\n",
              "      <td>0.413535655</td>\n",
              "      <td>0.5512923</td>\n",
              "      <td>0.381715655</td>\n",
              "      <td>0.495894909</td>\n",
              "      <td>0.386194438</td>\n",
              "      <td>0.497234821</td>\n",
              "      <td>0.429964721</td>\n",
              "      <td>0.494289339</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010997185</td>\n",
              "      <td>-0.524008512</td>\n",
              "      <td>-0.154198542</td>\n",
              "      <td>-0.020188037</td>\n",
              "      <td>0.020177683</td>\n",
              "      <td>-0.029583782</td>\n",
              "      <td>-0.306580335</td>\n",
              "      <td>0.362366378</td>\n",
              "      <td>-0.018393395</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3808</th>\n",
              "      <td>0.410130262</td>\n",
              "      <td>0.470612764</td>\n",
              "      <td>0.353161514</td>\n",
              "      <td>0.481383622</td>\n",
              "      <td>0.346614212</td>\n",
              "      <td>0.484839678</td>\n",
              "      <td>0.346404254</td>\n",
              "      <td>0.446524382</td>\n",
              "      <td>0.382068634</td>\n",
              "      <td>0.442739815</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.047307201</td>\n",
              "      <td>-0.20736447</td>\n",
              "      <td>-0.169882715</td>\n",
              "      <td>-0.009812003</td>\n",
              "      <td>0.009730845</td>\n",
              "      <td>-0.016656373</td>\n",
              "      <td>0.036118161</td>\n",
              "      <td>0.248286009</td>\n",
              "      <td>0.303717762</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3809</th>\n",
              "      <td>0.413310617</td>\n",
              "      <td>0.471635044</td>\n",
              "      <td>0.354795247</td>\n",
              "      <td>0.481459528</td>\n",
              "      <td>0.34364754</td>\n",
              "      <td>0.488360822</td>\n",
              "      <td>0.344700277</td>\n",
              "      <td>0.447434902</td>\n",
              "      <td>0.384895116</td>\n",
              "      <td>0.442575783</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.14865613</td>\n",
              "      <td>-0.192228019</td>\n",
              "      <td>-0.25464493</td>\n",
              "      <td>-0.000278628</td>\n",
              "      <td>0.00014017</td>\n",
              "      <td>-0.016059125</td>\n",
              "      <td>-0.000954395</td>\n",
              "      <td>0.230153367</td>\n",
              "      <td>0.250322253</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3810</th>\n",
              "      <td>0.425183952</td>\n",
              "      <td>0.478965431</td>\n",
              "      <td>0.361634165</td>\n",
              "      <td>0.487567008</td>\n",
              "      <td>0.350379795</td>\n",
              "      <td>0.493800342</td>\n",
              "      <td>0.350503743</td>\n",
              "      <td>0.453244925</td>\n",
              "      <td>0.391087115</td>\n",
              "      <td>0.453847826</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.069936648</td>\n",
              "      <td>-0.114805818</td>\n",
              "      <td>-0.167545915</td>\n",
              "      <td>0.005705269</td>\n",
              "      <td>-0.005720128</td>\n",
              "      <td>-0.020812653</td>\n",
              "      <td>-0.015095033</td>\n",
              "      <td>0.213164449</td>\n",
              "      <td>0.20433481</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3811</th>\n",
              "      <td>0.424494207</td>\n",
              "      <td>0.479368269</td>\n",
              "      <td>0.360941797</td>\n",
              "      <td>0.486462504</td>\n",
              "      <td>0.352958918</td>\n",
              "      <td>0.491775751</td>\n",
              "      <td>0.352376729</td>\n",
              "      <td>0.454283655</td>\n",
              "      <td>0.392247468</td>\n",
              "      <td>0.451816946</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.05203687</td>\n",
              "      <td>-0.222686455</td>\n",
              "      <td>-0.166772068</td>\n",
              "      <td>-0.00548711</td>\n",
              "      <td>0.005477537</td>\n",
              "      <td>0.011075634</td>\n",
              "      <td>0.045946468</td>\n",
              "      <td>0.289274305</td>\n",
              "      <td>0.309554756</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3812</th>\n",
              "      <td>0.415483415</td>\n",
              "      <td>0.472029865</td>\n",
              "      <td>0.358732998</td>\n",
              "      <td>0.484145254</td>\n",
              "      <td>0.349483103</td>\n",
              "      <td>0.49168691</td>\n",
              "      <td>0.348622948</td>\n",
              "      <td>0.453927696</td>\n",
              "      <td>0.391181767</td>\n",
              "      <td>0.451674372</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.110999048</td>\n",
              "      <td>-0.155995175</td>\n",
              "      <td>-0.221138313</td>\n",
              "      <td>0.005661204</td>\n",
              "      <td>-0.005736118</td>\n",
              "      <td>0.010790403</td>\n",
              "      <td>0.009689408</td>\n",
              "      <td>0.284101754</td>\n",
              "      <td>0.279072672</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3813 rows × 40 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ce3e218f-2357-49f4-a0bd-2be60977accf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ce3e218f-2357-49f4-a0bd-2be60977accf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ce3e218f-2357-49f4-a0bd-2be60977accf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_har.iloc[0]"
      ],
      "metadata": {
        "id": "cDK542f_21bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40936f58-624f-4826-a12f-a29c4c1f37fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0           nose_x\n",
              "1      l_shouldr_x\n",
              "2     r_shoulder_x\n",
              "3        l_elbow_x\n",
              "4        r_elbow_x\n",
              "5        l_wrist_x\n",
              "6        r_wrist_x\n",
              "7          l_hip_x\n",
              "8          r_hip_x\n",
              "9         l_knee_x\n",
              "10        r_knee_x\n",
              "11       l_ankle_x\n",
              "12       r_ankle_x\n",
              "13          nose_y\n",
              "14     l_shouldr_y\n",
              "15    r_shoulder_y\n",
              "16       l_elbow_y\n",
              "17       r_elbow_y\n",
              "18       l_wrist_y\n",
              "19       r_wrist_y\n",
              "20         l_hip_y\n",
              "21         r_hip_y\n",
              "22        l_knee_y\n",
              "23        r_knee_y\n",
              "24       l_ankle_y\n",
              "25       r_ankle_y\n",
              "26          nose_z\n",
              "27     l_shouldr_z\n",
              "28    r_shoulder_z\n",
              "29       l_elbow_z\n",
              "30       r_elbow_z\n",
              "31       l_wrist_z\n",
              "32       r_wrist_z\n",
              "33         l_hip_z\n",
              "34         r_hip_z\n",
              "35        l_knee_z\n",
              "36        r_knee_z\n",
              "37       l_ankle_z\n",
              "38       r_ankle_z\n",
              "39          Output\n",
              "Name: 0, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the first row\n",
        "#df_har.drop(0, axis= 0, inplace=True)\n",
        "#df_har\n",
        "\n",
        "\n",
        "# remove the first three columns and first row\n",
        "df = df_har.iloc[1: ,]\n",
        "df"
      ],
      "metadata": {
        "id": "Y6M54bqLHBIZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "c39f5e86-3da1-4de9-88e3-ffbee1d015e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               0            1            2            3            4   \\\n",
              "1     0.466836989  0.536425471  0.421737492  0.552586377  0.400636822   \n",
              "2     0.466981173  0.534423232  0.419800907  0.552579224  0.398616493   \n",
              "3      0.44387275  0.510528922   0.39275986  0.538784623  0.373896241   \n",
              "4     0.464225858  0.529767096  0.413535655    0.5512923  0.381715655   \n",
              "5      0.46530968  0.522170603  0.403040707  0.550458789  0.370451242   \n",
              "...           ...          ...          ...          ...          ...   \n",
              "3808  0.410130262  0.470612764  0.353161514  0.481383622  0.346614212   \n",
              "3809  0.413310617  0.471635044  0.354795247  0.481459528   0.34364754   \n",
              "3810  0.425183952  0.478965431  0.361634165  0.487567008  0.350379795   \n",
              "3811  0.424494207  0.479368269  0.360941797  0.486462504  0.352958918   \n",
              "3812  0.415483415  0.472029865  0.358732998  0.484145254  0.349483103   \n",
              "\n",
              "               5            6            7            8            9   ...  \\\n",
              "1     0.554411948  0.395507991  0.511357903  0.443766415  0.519800663  ...   \n",
              "2     0.550638914  0.395376265   0.51000458  0.442397594  0.518164456  ...   \n",
              "3     0.515839875  0.406628519  0.494533837  0.429210663  0.511657953  ...   \n",
              "4     0.495894909  0.386194438  0.497234821  0.429964721  0.494289339  ...   \n",
              "5     0.514875591  0.409228981  0.499619335  0.430802643  0.508876383  ...   \n",
              "...           ...          ...          ...          ...          ...  ...   \n",
              "3808  0.484839678  0.346404254  0.446524382  0.382068634  0.442739815  ...   \n",
              "3809  0.488360822  0.344700277  0.447434902  0.384895116  0.442575783  ...   \n",
              "3810  0.493800342  0.350503743  0.453244925  0.391087115  0.453847826  ...   \n",
              "3811  0.491775751  0.352376729  0.454283655  0.392247468  0.451816946  ...   \n",
              "3812   0.49168691  0.348622948  0.453927696  0.391181767  0.451674372  ...   \n",
              "\n",
              "                30            31            32            33            34  \\\n",
              "1     -0.127180368  -0.253910035  -0.229280859   -0.00820767   0.008048343   \n",
              "2     -0.134703189    -0.2186542  -0.265969336  -0.006716055   0.006530192   \n",
              "3     -0.233621523  -0.306831658   -0.44258517  -0.004903356   0.004764312   \n",
              "4      0.010997185  -0.524008512  -0.154198542  -0.020188037   0.020177683   \n",
              "5     -0.268270254  -0.437282801  -0.568175673    -0.0045435   0.004420654   \n",
              "...            ...           ...           ...           ...           ...   \n",
              "3808  -0.047307201   -0.20736447  -0.169882715  -0.009812003   0.009730845   \n",
              "3809   -0.14865613  -0.192228019   -0.25464493  -0.000278628    0.00014017   \n",
              "3810  -0.069936648  -0.114805818  -0.167545915   0.005705269  -0.005720128   \n",
              "3811   -0.05203687  -0.222686455  -0.166772068   -0.00548711   0.005477537   \n",
              "3812  -0.110999048  -0.155995175  -0.221138313   0.005661204  -0.005736118   \n",
              "\n",
              "                35            36           37            38 39  \n",
              "1      0.075822279   0.088045202   0.28819102   0.313930452  2  \n",
              "2      0.065338001   0.060155779  0.251547188   0.270694315  2  \n",
              "3      0.022069298   0.015744209  0.255176365   0.251044512  2  \n",
              "4     -0.029583782  -0.306580335  0.362366378  -0.018393395  2  \n",
              "5     -0.118991815   0.038211673  0.202935129   0.369386941  2  \n",
              "...            ...           ...          ...           ... ..  \n",
              "3808  -0.016656373   0.036118161  0.248286009   0.303717762  1  \n",
              "3809  -0.016059125  -0.000954395  0.230153367   0.250322253  1  \n",
              "3810  -0.020812653  -0.015095033  0.213164449    0.20433481  1  \n",
              "3811   0.011075634   0.045946468  0.289274305   0.309554756  1  \n",
              "3812   0.010790403   0.009689408  0.284101754   0.279072672  1  \n",
              "\n",
              "[3812 rows x 40 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b33e3b53-a9ff-4709-a21c-6bfcd617559d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.466836989</td>\n",
              "      <td>0.536425471</td>\n",
              "      <td>0.421737492</td>\n",
              "      <td>0.552586377</td>\n",
              "      <td>0.400636822</td>\n",
              "      <td>0.554411948</td>\n",
              "      <td>0.395507991</td>\n",
              "      <td>0.511357903</td>\n",
              "      <td>0.443766415</td>\n",
              "      <td>0.519800663</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.127180368</td>\n",
              "      <td>-0.253910035</td>\n",
              "      <td>-0.229280859</td>\n",
              "      <td>-0.00820767</td>\n",
              "      <td>0.008048343</td>\n",
              "      <td>0.075822279</td>\n",
              "      <td>0.088045202</td>\n",
              "      <td>0.28819102</td>\n",
              "      <td>0.313930452</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.466981173</td>\n",
              "      <td>0.534423232</td>\n",
              "      <td>0.419800907</td>\n",
              "      <td>0.552579224</td>\n",
              "      <td>0.398616493</td>\n",
              "      <td>0.550638914</td>\n",
              "      <td>0.395376265</td>\n",
              "      <td>0.51000458</td>\n",
              "      <td>0.442397594</td>\n",
              "      <td>0.518164456</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.134703189</td>\n",
              "      <td>-0.2186542</td>\n",
              "      <td>-0.265969336</td>\n",
              "      <td>-0.006716055</td>\n",
              "      <td>0.006530192</td>\n",
              "      <td>0.065338001</td>\n",
              "      <td>0.060155779</td>\n",
              "      <td>0.251547188</td>\n",
              "      <td>0.270694315</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.44387275</td>\n",
              "      <td>0.510528922</td>\n",
              "      <td>0.39275986</td>\n",
              "      <td>0.538784623</td>\n",
              "      <td>0.373896241</td>\n",
              "      <td>0.515839875</td>\n",
              "      <td>0.406628519</td>\n",
              "      <td>0.494533837</td>\n",
              "      <td>0.429210663</td>\n",
              "      <td>0.511657953</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.233621523</td>\n",
              "      <td>-0.306831658</td>\n",
              "      <td>-0.44258517</td>\n",
              "      <td>-0.004903356</td>\n",
              "      <td>0.004764312</td>\n",
              "      <td>0.022069298</td>\n",
              "      <td>0.015744209</td>\n",
              "      <td>0.255176365</td>\n",
              "      <td>0.251044512</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.464225858</td>\n",
              "      <td>0.529767096</td>\n",
              "      <td>0.413535655</td>\n",
              "      <td>0.5512923</td>\n",
              "      <td>0.381715655</td>\n",
              "      <td>0.495894909</td>\n",
              "      <td>0.386194438</td>\n",
              "      <td>0.497234821</td>\n",
              "      <td>0.429964721</td>\n",
              "      <td>0.494289339</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010997185</td>\n",
              "      <td>-0.524008512</td>\n",
              "      <td>-0.154198542</td>\n",
              "      <td>-0.020188037</td>\n",
              "      <td>0.020177683</td>\n",
              "      <td>-0.029583782</td>\n",
              "      <td>-0.306580335</td>\n",
              "      <td>0.362366378</td>\n",
              "      <td>-0.018393395</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.46530968</td>\n",
              "      <td>0.522170603</td>\n",
              "      <td>0.403040707</td>\n",
              "      <td>0.550458789</td>\n",
              "      <td>0.370451242</td>\n",
              "      <td>0.514875591</td>\n",
              "      <td>0.409228981</td>\n",
              "      <td>0.499619335</td>\n",
              "      <td>0.430802643</td>\n",
              "      <td>0.508876383</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.268270254</td>\n",
              "      <td>-0.437282801</td>\n",
              "      <td>-0.568175673</td>\n",
              "      <td>-0.0045435</td>\n",
              "      <td>0.004420654</td>\n",
              "      <td>-0.118991815</td>\n",
              "      <td>0.038211673</td>\n",
              "      <td>0.202935129</td>\n",
              "      <td>0.369386941</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3808</th>\n",
              "      <td>0.410130262</td>\n",
              "      <td>0.470612764</td>\n",
              "      <td>0.353161514</td>\n",
              "      <td>0.481383622</td>\n",
              "      <td>0.346614212</td>\n",
              "      <td>0.484839678</td>\n",
              "      <td>0.346404254</td>\n",
              "      <td>0.446524382</td>\n",
              "      <td>0.382068634</td>\n",
              "      <td>0.442739815</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.047307201</td>\n",
              "      <td>-0.20736447</td>\n",
              "      <td>-0.169882715</td>\n",
              "      <td>-0.009812003</td>\n",
              "      <td>0.009730845</td>\n",
              "      <td>-0.016656373</td>\n",
              "      <td>0.036118161</td>\n",
              "      <td>0.248286009</td>\n",
              "      <td>0.303717762</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3809</th>\n",
              "      <td>0.413310617</td>\n",
              "      <td>0.471635044</td>\n",
              "      <td>0.354795247</td>\n",
              "      <td>0.481459528</td>\n",
              "      <td>0.34364754</td>\n",
              "      <td>0.488360822</td>\n",
              "      <td>0.344700277</td>\n",
              "      <td>0.447434902</td>\n",
              "      <td>0.384895116</td>\n",
              "      <td>0.442575783</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.14865613</td>\n",
              "      <td>-0.192228019</td>\n",
              "      <td>-0.25464493</td>\n",
              "      <td>-0.000278628</td>\n",
              "      <td>0.00014017</td>\n",
              "      <td>-0.016059125</td>\n",
              "      <td>-0.000954395</td>\n",
              "      <td>0.230153367</td>\n",
              "      <td>0.250322253</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3810</th>\n",
              "      <td>0.425183952</td>\n",
              "      <td>0.478965431</td>\n",
              "      <td>0.361634165</td>\n",
              "      <td>0.487567008</td>\n",
              "      <td>0.350379795</td>\n",
              "      <td>0.493800342</td>\n",
              "      <td>0.350503743</td>\n",
              "      <td>0.453244925</td>\n",
              "      <td>0.391087115</td>\n",
              "      <td>0.453847826</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.069936648</td>\n",
              "      <td>-0.114805818</td>\n",
              "      <td>-0.167545915</td>\n",
              "      <td>0.005705269</td>\n",
              "      <td>-0.005720128</td>\n",
              "      <td>-0.020812653</td>\n",
              "      <td>-0.015095033</td>\n",
              "      <td>0.213164449</td>\n",
              "      <td>0.20433481</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3811</th>\n",
              "      <td>0.424494207</td>\n",
              "      <td>0.479368269</td>\n",
              "      <td>0.360941797</td>\n",
              "      <td>0.486462504</td>\n",
              "      <td>0.352958918</td>\n",
              "      <td>0.491775751</td>\n",
              "      <td>0.352376729</td>\n",
              "      <td>0.454283655</td>\n",
              "      <td>0.392247468</td>\n",
              "      <td>0.451816946</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.05203687</td>\n",
              "      <td>-0.222686455</td>\n",
              "      <td>-0.166772068</td>\n",
              "      <td>-0.00548711</td>\n",
              "      <td>0.005477537</td>\n",
              "      <td>0.011075634</td>\n",
              "      <td>0.045946468</td>\n",
              "      <td>0.289274305</td>\n",
              "      <td>0.309554756</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3812</th>\n",
              "      <td>0.415483415</td>\n",
              "      <td>0.472029865</td>\n",
              "      <td>0.358732998</td>\n",
              "      <td>0.484145254</td>\n",
              "      <td>0.349483103</td>\n",
              "      <td>0.49168691</td>\n",
              "      <td>0.348622948</td>\n",
              "      <td>0.453927696</td>\n",
              "      <td>0.391181767</td>\n",
              "      <td>0.451674372</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.110999048</td>\n",
              "      <td>-0.155995175</td>\n",
              "      <td>-0.221138313</td>\n",
              "      <td>0.005661204</td>\n",
              "      <td>-0.005736118</td>\n",
              "      <td>0.010790403</td>\n",
              "      <td>0.009689408</td>\n",
              "      <td>0.284101754</td>\n",
              "      <td>0.279072672</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3812 rows × 40 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b33e3b53-a9ff-4709-a21c-6bfcd617559d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b33e3b53-a9ff-4709-a21c-6bfcd617559d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b33e3b53-a9ff-4709-a21c-6bfcd617559d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = ['nose_x','l_shouldr_x','r_shoulder_x','l_elbow_x','r_elbow_x','l_wrist_x','r_wrist_x',\n",
        "          'l_hip_x','r_hip_x','l_knee_x','r_knee_x','l_ankle_x','r_ankle_x','nose_y','l_shouldr_y',\n",
        "          'r_shoulder_y','l_elbow_y','r_elbow_y','l_wrist_y','r_wrist_y','l_hip_y','r_hip_y','l_knee_y',\n",
        "          'r_knee_y','l_ankle_y','r_ankle_y','nose_z','l_shouldr_z','r_shoulder_z','l_elbow_z','r_elbow_z',\n",
        "          'l_wrist_z','r_wrist_z','l_hip_z','r_hip_z','l_knee_z','r_knee_z','l_ankle_z','r_ankle_z','Output']\n",
        "df"
      ],
      "metadata": {
        "id": "ouPS1pYWBvtt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "45f50598-ab0c-4652-aa71-1da3ac1b3c5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           nose_x  l_shouldr_x r_shoulder_x    l_elbow_x    r_elbow_x  \\\n",
              "1     0.466836989  0.536425471  0.421737492  0.552586377  0.400636822   \n",
              "2     0.466981173  0.534423232  0.419800907  0.552579224  0.398616493   \n",
              "3      0.44387275  0.510528922   0.39275986  0.538784623  0.373896241   \n",
              "4     0.464225858  0.529767096  0.413535655    0.5512923  0.381715655   \n",
              "5      0.46530968  0.522170603  0.403040707  0.550458789  0.370451242   \n",
              "...           ...          ...          ...          ...          ...   \n",
              "3808  0.410130262  0.470612764  0.353161514  0.481383622  0.346614212   \n",
              "3809  0.413310617  0.471635044  0.354795247  0.481459528   0.34364754   \n",
              "3810  0.425183952  0.478965431  0.361634165  0.487567008  0.350379795   \n",
              "3811  0.424494207  0.479368269  0.360941797  0.486462504  0.352958918   \n",
              "3812  0.415483415  0.472029865  0.358732998  0.484145254  0.349483103   \n",
              "\n",
              "        l_wrist_x    r_wrist_x      l_hip_x      r_hip_x     l_knee_x  ...  \\\n",
              "1     0.554411948  0.395507991  0.511357903  0.443766415  0.519800663  ...   \n",
              "2     0.550638914  0.395376265   0.51000458  0.442397594  0.518164456  ...   \n",
              "3     0.515839875  0.406628519  0.494533837  0.429210663  0.511657953  ...   \n",
              "4     0.495894909  0.386194438  0.497234821  0.429964721  0.494289339  ...   \n",
              "5     0.514875591  0.409228981  0.499619335  0.430802643  0.508876383  ...   \n",
              "...           ...          ...          ...          ...          ...  ...   \n",
              "3808  0.484839678  0.346404254  0.446524382  0.382068634  0.442739815  ...   \n",
              "3809  0.488360822  0.344700277  0.447434902  0.384895116  0.442575783  ...   \n",
              "3810  0.493800342  0.350503743  0.453244925  0.391087115  0.453847826  ...   \n",
              "3811  0.491775751  0.352376729  0.454283655  0.392247468  0.451816946  ...   \n",
              "3812   0.49168691  0.348622948  0.453927696  0.391181767  0.451674372  ...   \n",
              "\n",
              "         r_elbow_z     l_wrist_z     r_wrist_z       l_hip_z       r_hip_z  \\\n",
              "1     -0.127180368  -0.253910035  -0.229280859   -0.00820767   0.008048343   \n",
              "2     -0.134703189    -0.2186542  -0.265969336  -0.006716055   0.006530192   \n",
              "3     -0.233621523  -0.306831658   -0.44258517  -0.004903356   0.004764312   \n",
              "4      0.010997185  -0.524008512  -0.154198542  -0.020188037   0.020177683   \n",
              "5     -0.268270254  -0.437282801  -0.568175673    -0.0045435   0.004420654   \n",
              "...            ...           ...           ...           ...           ...   \n",
              "3808  -0.047307201   -0.20736447  -0.169882715  -0.009812003   0.009730845   \n",
              "3809   -0.14865613  -0.192228019   -0.25464493  -0.000278628    0.00014017   \n",
              "3810  -0.069936648  -0.114805818  -0.167545915   0.005705269  -0.005720128   \n",
              "3811   -0.05203687  -0.222686455  -0.166772068   -0.00548711   0.005477537   \n",
              "3812  -0.110999048  -0.155995175  -0.221138313   0.005661204  -0.005736118   \n",
              "\n",
              "          l_knee_z      r_knee_z    l_ankle_z     r_ankle_z Output  \n",
              "1      0.075822279   0.088045202   0.28819102   0.313930452      2  \n",
              "2      0.065338001   0.060155779  0.251547188   0.270694315      2  \n",
              "3      0.022069298   0.015744209  0.255176365   0.251044512      2  \n",
              "4     -0.029583782  -0.306580335  0.362366378  -0.018393395      2  \n",
              "5     -0.118991815   0.038211673  0.202935129   0.369386941      2  \n",
              "...            ...           ...          ...           ...    ...  \n",
              "3808  -0.016656373   0.036118161  0.248286009   0.303717762      1  \n",
              "3809  -0.016059125  -0.000954395  0.230153367   0.250322253      1  \n",
              "3810  -0.020812653  -0.015095033  0.213164449    0.20433481      1  \n",
              "3811   0.011075634   0.045946468  0.289274305   0.309554756      1  \n",
              "3812   0.010790403   0.009689408  0.284101754   0.279072672      1  \n",
              "\n",
              "[3812 rows x 40 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a106b18e-e2ad-4bf8-b858-84a35a4b6fed\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nose_x</th>\n",
              "      <th>l_shouldr_x</th>\n",
              "      <th>r_shoulder_x</th>\n",
              "      <th>l_elbow_x</th>\n",
              "      <th>r_elbow_x</th>\n",
              "      <th>l_wrist_x</th>\n",
              "      <th>r_wrist_x</th>\n",
              "      <th>l_hip_x</th>\n",
              "      <th>r_hip_x</th>\n",
              "      <th>l_knee_x</th>\n",
              "      <th>...</th>\n",
              "      <th>r_elbow_z</th>\n",
              "      <th>l_wrist_z</th>\n",
              "      <th>r_wrist_z</th>\n",
              "      <th>l_hip_z</th>\n",
              "      <th>r_hip_z</th>\n",
              "      <th>l_knee_z</th>\n",
              "      <th>r_knee_z</th>\n",
              "      <th>l_ankle_z</th>\n",
              "      <th>r_ankle_z</th>\n",
              "      <th>Output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.466836989</td>\n",
              "      <td>0.536425471</td>\n",
              "      <td>0.421737492</td>\n",
              "      <td>0.552586377</td>\n",
              "      <td>0.400636822</td>\n",
              "      <td>0.554411948</td>\n",
              "      <td>0.395507991</td>\n",
              "      <td>0.511357903</td>\n",
              "      <td>0.443766415</td>\n",
              "      <td>0.519800663</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.127180368</td>\n",
              "      <td>-0.253910035</td>\n",
              "      <td>-0.229280859</td>\n",
              "      <td>-0.00820767</td>\n",
              "      <td>0.008048343</td>\n",
              "      <td>0.075822279</td>\n",
              "      <td>0.088045202</td>\n",
              "      <td>0.28819102</td>\n",
              "      <td>0.313930452</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.466981173</td>\n",
              "      <td>0.534423232</td>\n",
              "      <td>0.419800907</td>\n",
              "      <td>0.552579224</td>\n",
              "      <td>0.398616493</td>\n",
              "      <td>0.550638914</td>\n",
              "      <td>0.395376265</td>\n",
              "      <td>0.51000458</td>\n",
              "      <td>0.442397594</td>\n",
              "      <td>0.518164456</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.134703189</td>\n",
              "      <td>-0.2186542</td>\n",
              "      <td>-0.265969336</td>\n",
              "      <td>-0.006716055</td>\n",
              "      <td>0.006530192</td>\n",
              "      <td>0.065338001</td>\n",
              "      <td>0.060155779</td>\n",
              "      <td>0.251547188</td>\n",
              "      <td>0.270694315</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.44387275</td>\n",
              "      <td>0.510528922</td>\n",
              "      <td>0.39275986</td>\n",
              "      <td>0.538784623</td>\n",
              "      <td>0.373896241</td>\n",
              "      <td>0.515839875</td>\n",
              "      <td>0.406628519</td>\n",
              "      <td>0.494533837</td>\n",
              "      <td>0.429210663</td>\n",
              "      <td>0.511657953</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.233621523</td>\n",
              "      <td>-0.306831658</td>\n",
              "      <td>-0.44258517</td>\n",
              "      <td>-0.004903356</td>\n",
              "      <td>0.004764312</td>\n",
              "      <td>0.022069298</td>\n",
              "      <td>0.015744209</td>\n",
              "      <td>0.255176365</td>\n",
              "      <td>0.251044512</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.464225858</td>\n",
              "      <td>0.529767096</td>\n",
              "      <td>0.413535655</td>\n",
              "      <td>0.5512923</td>\n",
              "      <td>0.381715655</td>\n",
              "      <td>0.495894909</td>\n",
              "      <td>0.386194438</td>\n",
              "      <td>0.497234821</td>\n",
              "      <td>0.429964721</td>\n",
              "      <td>0.494289339</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010997185</td>\n",
              "      <td>-0.524008512</td>\n",
              "      <td>-0.154198542</td>\n",
              "      <td>-0.020188037</td>\n",
              "      <td>0.020177683</td>\n",
              "      <td>-0.029583782</td>\n",
              "      <td>-0.306580335</td>\n",
              "      <td>0.362366378</td>\n",
              "      <td>-0.018393395</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.46530968</td>\n",
              "      <td>0.522170603</td>\n",
              "      <td>0.403040707</td>\n",
              "      <td>0.550458789</td>\n",
              "      <td>0.370451242</td>\n",
              "      <td>0.514875591</td>\n",
              "      <td>0.409228981</td>\n",
              "      <td>0.499619335</td>\n",
              "      <td>0.430802643</td>\n",
              "      <td>0.508876383</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.268270254</td>\n",
              "      <td>-0.437282801</td>\n",
              "      <td>-0.568175673</td>\n",
              "      <td>-0.0045435</td>\n",
              "      <td>0.004420654</td>\n",
              "      <td>-0.118991815</td>\n",
              "      <td>0.038211673</td>\n",
              "      <td>0.202935129</td>\n",
              "      <td>0.369386941</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3808</th>\n",
              "      <td>0.410130262</td>\n",
              "      <td>0.470612764</td>\n",
              "      <td>0.353161514</td>\n",
              "      <td>0.481383622</td>\n",
              "      <td>0.346614212</td>\n",
              "      <td>0.484839678</td>\n",
              "      <td>0.346404254</td>\n",
              "      <td>0.446524382</td>\n",
              "      <td>0.382068634</td>\n",
              "      <td>0.442739815</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.047307201</td>\n",
              "      <td>-0.20736447</td>\n",
              "      <td>-0.169882715</td>\n",
              "      <td>-0.009812003</td>\n",
              "      <td>0.009730845</td>\n",
              "      <td>-0.016656373</td>\n",
              "      <td>0.036118161</td>\n",
              "      <td>0.248286009</td>\n",
              "      <td>0.303717762</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3809</th>\n",
              "      <td>0.413310617</td>\n",
              "      <td>0.471635044</td>\n",
              "      <td>0.354795247</td>\n",
              "      <td>0.481459528</td>\n",
              "      <td>0.34364754</td>\n",
              "      <td>0.488360822</td>\n",
              "      <td>0.344700277</td>\n",
              "      <td>0.447434902</td>\n",
              "      <td>0.384895116</td>\n",
              "      <td>0.442575783</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.14865613</td>\n",
              "      <td>-0.192228019</td>\n",
              "      <td>-0.25464493</td>\n",
              "      <td>-0.000278628</td>\n",
              "      <td>0.00014017</td>\n",
              "      <td>-0.016059125</td>\n",
              "      <td>-0.000954395</td>\n",
              "      <td>0.230153367</td>\n",
              "      <td>0.250322253</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3810</th>\n",
              "      <td>0.425183952</td>\n",
              "      <td>0.478965431</td>\n",
              "      <td>0.361634165</td>\n",
              "      <td>0.487567008</td>\n",
              "      <td>0.350379795</td>\n",
              "      <td>0.493800342</td>\n",
              "      <td>0.350503743</td>\n",
              "      <td>0.453244925</td>\n",
              "      <td>0.391087115</td>\n",
              "      <td>0.453847826</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.069936648</td>\n",
              "      <td>-0.114805818</td>\n",
              "      <td>-0.167545915</td>\n",
              "      <td>0.005705269</td>\n",
              "      <td>-0.005720128</td>\n",
              "      <td>-0.020812653</td>\n",
              "      <td>-0.015095033</td>\n",
              "      <td>0.213164449</td>\n",
              "      <td>0.20433481</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3811</th>\n",
              "      <td>0.424494207</td>\n",
              "      <td>0.479368269</td>\n",
              "      <td>0.360941797</td>\n",
              "      <td>0.486462504</td>\n",
              "      <td>0.352958918</td>\n",
              "      <td>0.491775751</td>\n",
              "      <td>0.352376729</td>\n",
              "      <td>0.454283655</td>\n",
              "      <td>0.392247468</td>\n",
              "      <td>0.451816946</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.05203687</td>\n",
              "      <td>-0.222686455</td>\n",
              "      <td>-0.166772068</td>\n",
              "      <td>-0.00548711</td>\n",
              "      <td>0.005477537</td>\n",
              "      <td>0.011075634</td>\n",
              "      <td>0.045946468</td>\n",
              "      <td>0.289274305</td>\n",
              "      <td>0.309554756</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3812</th>\n",
              "      <td>0.415483415</td>\n",
              "      <td>0.472029865</td>\n",
              "      <td>0.358732998</td>\n",
              "      <td>0.484145254</td>\n",
              "      <td>0.349483103</td>\n",
              "      <td>0.49168691</td>\n",
              "      <td>0.348622948</td>\n",
              "      <td>0.453927696</td>\n",
              "      <td>0.391181767</td>\n",
              "      <td>0.451674372</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.110999048</td>\n",
              "      <td>-0.155995175</td>\n",
              "      <td>-0.221138313</td>\n",
              "      <td>0.005661204</td>\n",
              "      <td>-0.005736118</td>\n",
              "      <td>0.010790403</td>\n",
              "      <td>0.009689408</td>\n",
              "      <td>0.284101754</td>\n",
              "      <td>0.279072672</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3812 rows × 40 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a106b18e-e2ad-4bf8-b858-84a35a4b6fed')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a106b18e-e2ad-4bf8-b858-84a35a4b6fed button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a106b18e-e2ad-4bf8-b858-84a35a4b6fed');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# removing null values\n",
        "df = df.dropna()\n",
        "df.shape"
      ],
      "metadata": {
        "id": "0O-uXgtgHxiA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36e7b151-fa42-4b2c-a572-e7cb456f4846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3812, 40)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_style(\"darkgrid\")\n",
        "plt.figure(figsize = (10, 5))\n",
        "sns.countplot(x = \"Output\", data = df)\n",
        "plt.title(\"Number of samples by activity\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J2hJ5S_9JxHQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "273146f6-bb60-43db-8fab-63c8561759b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "activities = df['Output'].value_counts().index\n",
        "activities\n"
      ],
      "metadata": {
        "id": "bcuS6og6Q66s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "177c40e0-3517-431c-8bbd-c910eff72206"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['4', '1', '0', '3', '2', '5'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "label = LabelEncoder()\n",
        "df['label'] = label.fit_transform(df['Output'])\n",
        "df\n"
      ],
      "metadata": {
        "id": "5-aYKuVeTiu0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "94202c6e-762c-43ea-c84d-41847bfc02b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           nose_x  l_shouldr_x r_shoulder_x    l_elbow_x    r_elbow_x  \\\n",
              "1     0.466836989  0.536425471  0.421737492  0.552586377  0.400636822   \n",
              "2     0.466981173  0.534423232  0.419800907  0.552579224  0.398616493   \n",
              "3      0.44387275  0.510528922   0.39275986  0.538784623  0.373896241   \n",
              "4     0.464225858  0.529767096  0.413535655    0.5512923  0.381715655   \n",
              "5      0.46530968  0.522170603  0.403040707  0.550458789  0.370451242   \n",
              "...           ...          ...          ...          ...          ...   \n",
              "3808  0.410130262  0.470612764  0.353161514  0.481383622  0.346614212   \n",
              "3809  0.413310617  0.471635044  0.354795247  0.481459528   0.34364754   \n",
              "3810  0.425183952  0.478965431  0.361634165  0.487567008  0.350379795   \n",
              "3811  0.424494207  0.479368269  0.360941797  0.486462504  0.352958918   \n",
              "3812  0.415483415  0.472029865  0.358732998  0.484145254  0.349483103   \n",
              "\n",
              "        l_wrist_x    r_wrist_x      l_hip_x      r_hip_x     l_knee_x  ...  \\\n",
              "1     0.554411948  0.395507991  0.511357903  0.443766415  0.519800663  ...   \n",
              "2     0.550638914  0.395376265   0.51000458  0.442397594  0.518164456  ...   \n",
              "3     0.515839875  0.406628519  0.494533837  0.429210663  0.511657953  ...   \n",
              "4     0.495894909  0.386194438  0.497234821  0.429964721  0.494289339  ...   \n",
              "5     0.514875591  0.409228981  0.499619335  0.430802643  0.508876383  ...   \n",
              "...           ...          ...          ...          ...          ...  ...   \n",
              "3808  0.484839678  0.346404254  0.446524382  0.382068634  0.442739815  ...   \n",
              "3809  0.488360822  0.344700277  0.447434902  0.384895116  0.442575783  ...   \n",
              "3810  0.493800342  0.350503743  0.453244925  0.391087115  0.453847826  ...   \n",
              "3811  0.491775751  0.352376729  0.454283655  0.392247468  0.451816946  ...   \n",
              "3812   0.49168691  0.348622948  0.453927696  0.391181767  0.451674372  ...   \n",
              "\n",
              "         l_wrist_z     r_wrist_z       l_hip_z       r_hip_z      l_knee_z  \\\n",
              "1     -0.253910035  -0.229280859   -0.00820767   0.008048343   0.075822279   \n",
              "2       -0.2186542  -0.265969336  -0.006716055   0.006530192   0.065338001   \n",
              "3     -0.306831658   -0.44258517  -0.004903356   0.004764312   0.022069298   \n",
              "4     -0.524008512  -0.154198542  -0.020188037   0.020177683  -0.029583782   \n",
              "5     -0.437282801  -0.568175673    -0.0045435   0.004420654  -0.118991815   \n",
              "...            ...           ...           ...           ...           ...   \n",
              "3808   -0.20736447  -0.169882715  -0.009812003   0.009730845  -0.016656373   \n",
              "3809  -0.192228019   -0.25464493  -0.000278628    0.00014017  -0.016059125   \n",
              "3810  -0.114805818  -0.167545915   0.005705269  -0.005720128  -0.020812653   \n",
              "3811  -0.222686455  -0.166772068   -0.00548711   0.005477537   0.011075634   \n",
              "3812  -0.155995175  -0.221138313   0.005661204  -0.005736118   0.010790403   \n",
              "\n",
              "          r_knee_z    l_ankle_z     r_ankle_z Output label  \n",
              "1      0.088045202   0.28819102   0.313930452      2     2  \n",
              "2      0.060155779  0.251547188   0.270694315      2     2  \n",
              "3      0.015744209  0.255176365   0.251044512      2     2  \n",
              "4     -0.306580335  0.362366378  -0.018393395      2     2  \n",
              "5      0.038211673  0.202935129   0.369386941      2     2  \n",
              "...            ...          ...           ...    ...   ...  \n",
              "3808   0.036118161  0.248286009   0.303717762      1     1  \n",
              "3809  -0.000954395  0.230153367   0.250322253      1     1  \n",
              "3810  -0.015095033  0.213164449    0.20433481      1     1  \n",
              "3811   0.045946468  0.289274305   0.309554756      1     1  \n",
              "3812   0.009689408  0.284101754   0.279072672      1     1  \n",
              "\n",
              "[3812 rows x 41 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-14a72d29-4084-4b9e-b66e-556f66799d8f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nose_x</th>\n",
              "      <th>l_shouldr_x</th>\n",
              "      <th>r_shoulder_x</th>\n",
              "      <th>l_elbow_x</th>\n",
              "      <th>r_elbow_x</th>\n",
              "      <th>l_wrist_x</th>\n",
              "      <th>r_wrist_x</th>\n",
              "      <th>l_hip_x</th>\n",
              "      <th>r_hip_x</th>\n",
              "      <th>l_knee_x</th>\n",
              "      <th>...</th>\n",
              "      <th>l_wrist_z</th>\n",
              "      <th>r_wrist_z</th>\n",
              "      <th>l_hip_z</th>\n",
              "      <th>r_hip_z</th>\n",
              "      <th>l_knee_z</th>\n",
              "      <th>r_knee_z</th>\n",
              "      <th>l_ankle_z</th>\n",
              "      <th>r_ankle_z</th>\n",
              "      <th>Output</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.466836989</td>\n",
              "      <td>0.536425471</td>\n",
              "      <td>0.421737492</td>\n",
              "      <td>0.552586377</td>\n",
              "      <td>0.400636822</td>\n",
              "      <td>0.554411948</td>\n",
              "      <td>0.395507991</td>\n",
              "      <td>0.511357903</td>\n",
              "      <td>0.443766415</td>\n",
              "      <td>0.519800663</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.253910035</td>\n",
              "      <td>-0.229280859</td>\n",
              "      <td>-0.00820767</td>\n",
              "      <td>0.008048343</td>\n",
              "      <td>0.075822279</td>\n",
              "      <td>0.088045202</td>\n",
              "      <td>0.28819102</td>\n",
              "      <td>0.313930452</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.466981173</td>\n",
              "      <td>0.534423232</td>\n",
              "      <td>0.419800907</td>\n",
              "      <td>0.552579224</td>\n",
              "      <td>0.398616493</td>\n",
              "      <td>0.550638914</td>\n",
              "      <td>0.395376265</td>\n",
              "      <td>0.51000458</td>\n",
              "      <td>0.442397594</td>\n",
              "      <td>0.518164456</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.2186542</td>\n",
              "      <td>-0.265969336</td>\n",
              "      <td>-0.006716055</td>\n",
              "      <td>0.006530192</td>\n",
              "      <td>0.065338001</td>\n",
              "      <td>0.060155779</td>\n",
              "      <td>0.251547188</td>\n",
              "      <td>0.270694315</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.44387275</td>\n",
              "      <td>0.510528922</td>\n",
              "      <td>0.39275986</td>\n",
              "      <td>0.538784623</td>\n",
              "      <td>0.373896241</td>\n",
              "      <td>0.515839875</td>\n",
              "      <td>0.406628519</td>\n",
              "      <td>0.494533837</td>\n",
              "      <td>0.429210663</td>\n",
              "      <td>0.511657953</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.306831658</td>\n",
              "      <td>-0.44258517</td>\n",
              "      <td>-0.004903356</td>\n",
              "      <td>0.004764312</td>\n",
              "      <td>0.022069298</td>\n",
              "      <td>0.015744209</td>\n",
              "      <td>0.255176365</td>\n",
              "      <td>0.251044512</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.464225858</td>\n",
              "      <td>0.529767096</td>\n",
              "      <td>0.413535655</td>\n",
              "      <td>0.5512923</td>\n",
              "      <td>0.381715655</td>\n",
              "      <td>0.495894909</td>\n",
              "      <td>0.386194438</td>\n",
              "      <td>0.497234821</td>\n",
              "      <td>0.429964721</td>\n",
              "      <td>0.494289339</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.524008512</td>\n",
              "      <td>-0.154198542</td>\n",
              "      <td>-0.020188037</td>\n",
              "      <td>0.020177683</td>\n",
              "      <td>-0.029583782</td>\n",
              "      <td>-0.306580335</td>\n",
              "      <td>0.362366378</td>\n",
              "      <td>-0.018393395</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.46530968</td>\n",
              "      <td>0.522170603</td>\n",
              "      <td>0.403040707</td>\n",
              "      <td>0.550458789</td>\n",
              "      <td>0.370451242</td>\n",
              "      <td>0.514875591</td>\n",
              "      <td>0.409228981</td>\n",
              "      <td>0.499619335</td>\n",
              "      <td>0.430802643</td>\n",
              "      <td>0.508876383</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.437282801</td>\n",
              "      <td>-0.568175673</td>\n",
              "      <td>-0.0045435</td>\n",
              "      <td>0.004420654</td>\n",
              "      <td>-0.118991815</td>\n",
              "      <td>0.038211673</td>\n",
              "      <td>0.202935129</td>\n",
              "      <td>0.369386941</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3808</th>\n",
              "      <td>0.410130262</td>\n",
              "      <td>0.470612764</td>\n",
              "      <td>0.353161514</td>\n",
              "      <td>0.481383622</td>\n",
              "      <td>0.346614212</td>\n",
              "      <td>0.484839678</td>\n",
              "      <td>0.346404254</td>\n",
              "      <td>0.446524382</td>\n",
              "      <td>0.382068634</td>\n",
              "      <td>0.442739815</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.20736447</td>\n",
              "      <td>-0.169882715</td>\n",
              "      <td>-0.009812003</td>\n",
              "      <td>0.009730845</td>\n",
              "      <td>-0.016656373</td>\n",
              "      <td>0.036118161</td>\n",
              "      <td>0.248286009</td>\n",
              "      <td>0.303717762</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3809</th>\n",
              "      <td>0.413310617</td>\n",
              "      <td>0.471635044</td>\n",
              "      <td>0.354795247</td>\n",
              "      <td>0.481459528</td>\n",
              "      <td>0.34364754</td>\n",
              "      <td>0.488360822</td>\n",
              "      <td>0.344700277</td>\n",
              "      <td>0.447434902</td>\n",
              "      <td>0.384895116</td>\n",
              "      <td>0.442575783</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.192228019</td>\n",
              "      <td>-0.25464493</td>\n",
              "      <td>-0.000278628</td>\n",
              "      <td>0.00014017</td>\n",
              "      <td>-0.016059125</td>\n",
              "      <td>-0.000954395</td>\n",
              "      <td>0.230153367</td>\n",
              "      <td>0.250322253</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3810</th>\n",
              "      <td>0.425183952</td>\n",
              "      <td>0.478965431</td>\n",
              "      <td>0.361634165</td>\n",
              "      <td>0.487567008</td>\n",
              "      <td>0.350379795</td>\n",
              "      <td>0.493800342</td>\n",
              "      <td>0.350503743</td>\n",
              "      <td>0.453244925</td>\n",
              "      <td>0.391087115</td>\n",
              "      <td>0.453847826</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.114805818</td>\n",
              "      <td>-0.167545915</td>\n",
              "      <td>0.005705269</td>\n",
              "      <td>-0.005720128</td>\n",
              "      <td>-0.020812653</td>\n",
              "      <td>-0.015095033</td>\n",
              "      <td>0.213164449</td>\n",
              "      <td>0.20433481</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3811</th>\n",
              "      <td>0.424494207</td>\n",
              "      <td>0.479368269</td>\n",
              "      <td>0.360941797</td>\n",
              "      <td>0.486462504</td>\n",
              "      <td>0.352958918</td>\n",
              "      <td>0.491775751</td>\n",
              "      <td>0.352376729</td>\n",
              "      <td>0.454283655</td>\n",
              "      <td>0.392247468</td>\n",
              "      <td>0.451816946</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.222686455</td>\n",
              "      <td>-0.166772068</td>\n",
              "      <td>-0.00548711</td>\n",
              "      <td>0.005477537</td>\n",
              "      <td>0.011075634</td>\n",
              "      <td>0.045946468</td>\n",
              "      <td>0.289274305</td>\n",
              "      <td>0.309554756</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3812</th>\n",
              "      <td>0.415483415</td>\n",
              "      <td>0.472029865</td>\n",
              "      <td>0.358732998</td>\n",
              "      <td>0.484145254</td>\n",
              "      <td>0.349483103</td>\n",
              "      <td>0.49168691</td>\n",
              "      <td>0.348622948</td>\n",
              "      <td>0.453927696</td>\n",
              "      <td>0.391181767</td>\n",
              "      <td>0.451674372</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.155995175</td>\n",
              "      <td>-0.221138313</td>\n",
              "      <td>0.005661204</td>\n",
              "      <td>-0.005736118</td>\n",
              "      <td>0.010790403</td>\n",
              "      <td>0.009689408</td>\n",
              "      <td>0.284101754</td>\n",
              "      <td>0.279072672</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3812 rows × 41 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-14a72d29-4084-4b9e-b66e-556f66799d8f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-14a72d29-4084-4b9e-b66e-556f66799d8f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-14a72d29-4084-4b9e-b66e-556f66799d8f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frame Preparation"
      ],
      "metadata": {
        "id": "625O_NFFWGBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['Output'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "B4oufHxvcx19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.astype(float)"
      ],
      "metadata": {
        "id": "LRJtzt1DdT4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "XJ-JkMNbdYx7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d32dd1f-b9f5-4319-ecc6-b4aeaeee053d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nose_x          float64\n",
              "l_shouldr_x     float64\n",
              "r_shoulder_x    float64\n",
              "l_elbow_x       float64\n",
              "r_elbow_x       float64\n",
              "l_wrist_x       float64\n",
              "r_wrist_x       float64\n",
              "l_hip_x         float64\n",
              "r_hip_x         float64\n",
              "l_knee_x        float64\n",
              "r_knee_x        float64\n",
              "l_ankle_x       float64\n",
              "r_ankle_x       float64\n",
              "nose_y          float64\n",
              "l_shouldr_y     float64\n",
              "r_shoulder_y    float64\n",
              "l_elbow_y       float64\n",
              "r_elbow_y       float64\n",
              "l_wrist_y       float64\n",
              "r_wrist_y       float64\n",
              "l_hip_y         float64\n",
              "r_hip_y         float64\n",
              "l_knee_y        float64\n",
              "r_knee_y        float64\n",
              "l_ankle_y       float64\n",
              "r_ankle_y       float64\n",
              "nose_z          float64\n",
              "l_shouldr_z     float64\n",
              "r_shoulder_z    float64\n",
              "l_elbow_z       float64\n",
              "r_elbow_z       float64\n",
              "l_wrist_z       float64\n",
              "r_wrist_z       float64\n",
              "l_hip_z         float64\n",
              "r_hip_z         float64\n",
              "l_knee_z        float64\n",
              "r_knee_z        float64\n",
              "l_ankle_z       float64\n",
              "r_ankle_z       float64\n",
              "label           float64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats"
      ],
      "metadata": {
        "id": "agAxlzv6VrPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Fs = 50\n",
        "frame_size = 6\n",
        "hop_size = 1"
      ],
      "metadata": {
        "id": "uEcnkRJKWmp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_windows(df, frame_size, hop_size):\n",
        "\n",
        "    N_FEATURES = 39\n",
        "\n",
        "    frames = []\n",
        "    labels = []\n",
        "    for i in range(0, len(df) - frame_size, hop_size):\n",
        "        aA = df['nose_x'].values[i: i + frame_size]\n",
        "        aB = df['l_shouldr_x'].values[i: i + frame_size]\n",
        "        aC = df['r_shoulder_x'].values[i: i + frame_size]\n",
        "        aD = df['l_elbow_x'].values[i: i + frame_size]\n",
        "        aE = df['r_elbow_x'].values[i: i + frame_size]\n",
        "        aF = df['l_wrist_x'].values[i: i + frame_size]\n",
        "        aG = df['r_wrist_x'].values[i: i + frame_size]\n",
        "        aH = df['l_hip_x'].values[i: i + frame_size]\n",
        "        aI = df['r_hip_x'].values[i: i + frame_size]\n",
        "        aJ = df['l_knee_x'].values[i: i + frame_size]\n",
        "        aK = df['r_knee_x'].values[i: i + frame_size]\n",
        "        aL = df['l_ankle_x'].values[i: i + frame_size]\n",
        "        aM = df['r_ankle_x'].values[i: i + frame_size]\n",
        "        aN = df['nose_y'].values[i: i + frame_size]\n",
        "        aO = df['l_shouldr_y'].values[i: i + frame_size]\n",
        "        aP = df['r_shoulder_y'].values[i: i + frame_size]\n",
        "        aQ = df['l_elbow_y'].values[i: i + frame_size]\n",
        "        aR = df['r_elbow_y'].values[i: i + frame_size]\n",
        "        aS = df['l_wrist_y'].values[i: i + frame_size]\n",
        "        aT = df['r_wrist_y'].values[i: i + frame_size]\n",
        "        aU = df['l_hip_y'].values[i: i + frame_size]\n",
        "        aV = df['r_hip_y'].values[i: i + frame_size]\n",
        "        aW = df['l_knee_y'].values[i: i + frame_size]\n",
        "        aX = df['r_knee_y'].values[i: i + frame_size]\n",
        "        aY = df['l_ankle_y'].values[i: i + frame_size]\n",
        "        aZ = df['r_ankle_y'].values[i: i + frame_size]\n",
        "        bA = df['nose_z'].values[i: i + frame_size]\n",
        "        bB = df['l_shouldr_z'].values[i: i + frame_size]\n",
        "        bC = df['r_shoulder_z'].values[i: i + frame_size]\n",
        "        bD = df['l_elbow_z'].values[i: i + frame_size]\n",
        "        bE = df['r_elbow_z'].values[i: i + frame_size]\n",
        "        bF = df['l_wrist_z'].values[i: i + frame_size]\n",
        "        bG = df['r_wrist_z'].values[i: i + frame_size]\n",
        "        bH = df['l_hip_z'].values[i: i + frame_size]\n",
        "        bI = df['r_hip_z'].values[i: i + frame_size]\n",
        "        bJ = df['l_knee_z'].values[i: i + frame_size]\n",
        "        bK = df['r_knee_z'].values[i: i + frame_size]\n",
        "        bL = df['l_ankle_z'].values[i: i + frame_size]\n",
        "        bM = df['r_ankle_z'].values[i: i + frame_size]\n",
        "        \n",
        "        # Retrieve the most often used label in this segment\n",
        "        label = stats.mode(df['label'][i: i + frame_size])[0][0]\n",
        "        frames.append([aA, aB, aC, aD, aE, aF, aG, aH, aI, aJ, aK, aL, aM, aN, aO, aP, aQ, aR, aS, aT, aU, aV, aW, aX, aY, aZ, bA, bB, bC, bD, bE, bF, bG, bH, bI, bJ, bK, bL, bM])\n",
        "        labels.append(label)\n",
        "\n",
        "    # Bring the segments into a better shape\n",
        "    frames = np.asarray(frames).reshape(-1, frame_size, N_FEATURES)\n",
        "    labels = np.asarray(labels)\n",
        "\n",
        "    return frames, labels"
      ],
      "metadata": {
        "id": "VsS3SOZ8h7pS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = get_windows(df, frame_size, hop_size)"
      ],
      "metadata": {
        "id": "QjKMGdt7kWGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "9Fn3xzJBSODI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6935373-1b9f-4343-acfe-e3cdc7555e68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nose_x          float64\n",
              "l_shouldr_x     float64\n",
              "r_shoulder_x    float64\n",
              "l_elbow_x       float64\n",
              "r_elbow_x       float64\n",
              "l_wrist_x       float64\n",
              "r_wrist_x       float64\n",
              "l_hip_x         float64\n",
              "r_hip_x         float64\n",
              "l_knee_x        float64\n",
              "r_knee_x        float64\n",
              "l_ankle_x       float64\n",
              "r_ankle_x       float64\n",
              "nose_y          float64\n",
              "l_shouldr_y     float64\n",
              "r_shoulder_y    float64\n",
              "l_elbow_y       float64\n",
              "r_elbow_y       float64\n",
              "l_wrist_y       float64\n",
              "r_wrist_y       float64\n",
              "l_hip_y         float64\n",
              "r_hip_y         float64\n",
              "l_knee_y        float64\n",
              "r_knee_y        float64\n",
              "l_ankle_y       float64\n",
              "r_ankle_y       float64\n",
              "nose_z          float64\n",
              "l_shouldr_z     float64\n",
              "r_shoulder_z    float64\n",
              "l_elbow_z       float64\n",
              "r_elbow_z       float64\n",
              "l_wrist_z       float64\n",
              "r_wrist_z       float64\n",
              "l_hip_z         float64\n",
              "r_hip_z         float64\n",
              "l_knee_z        float64\n",
              "r_knee_z        float64\n",
              "l_ankle_z       float64\n",
              "r_ankle_z       float64\n",
              "label           float64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape, y.shape"
      ],
      "metadata": {
        "id": "oQZg0O9iYp50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5491139e-d043-42af-c32a-07e7c3451299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3806, 6, 39), (3806,))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "fg3RZQXXZQ8-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bffd109d-8004-4d83-daa2-d0f9c9d563cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[ 0.46683699,  0.46698117,  0.44387275, ...,  0.39550799,\n",
              "          0.39537627,  0.40662852],\n",
              "        [ 0.38619444,  0.40922898,  0.38588965, ...,  0.42773223,\n",
              "          0.44640669,  0.43202224],\n",
              "        [ 0.12364558,  0.11725052,  0.11981739, ...,  0.42785987,\n",
              "          0.42614666,  0.365944  ],\n",
              "        [ 0.37724251,  0.29704192,  0.3552956 , ...,  0.6846683 ,\n",
              "          0.70797843,  0.69003618],\n",
              "        [-0.39526755, -0.3458443 , -0.44058686, ..., -0.22928086,\n",
              "         -0.26596934, -0.44258517],\n",
              "        [-0.15419854, -0.56817567, -0.11683458, ..., -0.01839339,\n",
              "          0.36938694,  0.02450635]],\n",
              "\n",
              "       [[ 0.46698117,  0.44387275,  0.46422586, ...,  0.39537627,\n",
              "          0.40662852,  0.38619444],\n",
              "        [ 0.40922898,  0.38588965,  0.408943  , ...,  0.44640669,\n",
              "          0.43202224,  0.43895808],\n",
              "        [ 0.11725052,  0.11981739,  0.13492356, ...,  0.42614666,\n",
              "          0.365944  ,  0.37724251],\n",
              "        [ 0.29704192,  0.3552956 ,  0.27939203, ...,  0.70797843,\n",
              "          0.69003618,  0.71701962],\n",
              "        [-0.3458443 , -0.44058686, -0.33217534, ..., -0.26596934,\n",
              "         -0.44258517, -0.15419854],\n",
              "        [-0.56817567, -0.11683458, -0.46314907, ...,  0.36938694,\n",
              "          0.02450635,  0.24189119]],\n",
              "\n",
              "       [[ 0.44387275,  0.46422586,  0.46530968, ...,  0.40662852,\n",
              "          0.38619444,  0.40922898],\n",
              "        [ 0.38588965,  0.408943  ,  0.39879623, ...,  0.43202224,\n",
              "          0.43895808,  0.43650001],\n",
              "        [ 0.11981739,  0.13492356,  0.12460114, ...,  0.365944  ,\n",
              "          0.37724251,  0.29704192],\n",
              "        [ 0.3552956 ,  0.27939203,  0.33681044, ...,  0.69003618,\n",
              "          0.71701962,  0.70908642],\n",
              "        [-0.44058686, -0.33217534, -0.43176538, ..., -0.44258517,\n",
              "         -0.15419854, -0.56817567],\n",
              "        [-0.11683458, -0.46314907, -0.37233096, ...,  0.02450635,\n",
              "          0.24189119,  0.31932172]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 0.43434626,  0.42984033,  0.42244038, ...,  0.36272025,\n",
              "          0.3696579 ,  0.35426238],\n",
              "        [ 0.3474662 ,  0.34640425,  0.34470028, ...,  0.40598178,\n",
              "          0.39587727,  0.39121854],\n",
              "        [ 0.12981899,  0.17940181,  0.23077838, ...,  0.48149732,\n",
              "          0.48772827,  0.54459202],\n",
              "        [ 0.5605579 ,  0.52843821,  0.4906334 , ...,  0.88416314,\n",
              "          0.82782894,  0.82440287],\n",
              "        [-0.41887078, -0.44403163, -0.3610431 , ..., -0.2185446 ,\n",
              "         -0.19271657, -0.1809172 ],\n",
              "        [-0.13922366, -0.16988271, -0.25464493, ...,  0.24631488,\n",
              "          0.30371776,  0.25032225]],\n",
              "\n",
              "       [[ 0.42984033,  0.42244038,  0.40300193, ...,  0.3696579 ,\n",
              "          0.35426238,  0.3474662 ],\n",
              "        [ 0.34640425,  0.34470028,  0.35050374, ...,  0.39587727,\n",
              "          0.39121854,  0.41332546],\n",
              "        [ 0.17940181,  0.23077838,  0.20091996, ...,  0.48772827,\n",
              "          0.54459202,  0.5605579 ],\n",
              "        [ 0.52843821,  0.4906334 ,  0.49073732, ...,  0.82782894,\n",
              "          0.82440287,  0.8435623 ],\n",
              "        [-0.44403163, -0.3610431 , -0.37452132, ..., -0.19271657,\n",
              "         -0.1809172 , -0.13922366],\n",
              "        [-0.16988271, -0.25464493, -0.16754591, ...,  0.30371776,\n",
              "          0.25032225,  0.20433481]],\n",
              "\n",
              "       [[ 0.42244038,  0.40300193,  0.41013026, ...,  0.35426238,\n",
              "          0.3474662 ,  0.34640425],\n",
              "        [ 0.34470028,  0.35050374,  0.35237673, ...,  0.39121854,\n",
              "          0.41332546,  0.41795841],\n",
              "        [ 0.23077838,  0.20091996,  0.19220927, ...,  0.54459202,\n",
              "          0.5605579 ,  0.52843821],\n",
              "        [ 0.4906334 ,  0.49073732,  0.56137276, ...,  0.82440287,\n",
              "          0.8435623 ,  0.86171019],\n",
              "        [-0.3610431 , -0.37452132, -0.37311357, ..., -0.1809172 ,\n",
              "         -0.13922366, -0.16988271],\n",
              "        [-0.25464493, -0.16754591, -0.16677207, ...,  0.25032225,\n",
              "          0.20433481,  0.30955476]]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA PREPROCESSING"
      ],
      "metadata": {
        "id": "9_zsw65A81Dw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "q1-aBpbCGsi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "vW2S1GKyGuCU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a5df7ed-6112-484c-943c-d6f2f6b95200"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42,shuffle=True)"
      ],
      "metadata": {
        "id": "bydoxomm8vDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "6rsGuAHgdqU0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f8cdc79-5c05-4249-d7af-d82d991f1a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[ 0.46445185,  0.45988926,  0.45297632, ...,  0.37362778,\n",
              "          0.39557359,  0.39281651],\n",
              "        [ 0.37032092,  0.37335175,  0.38296375, ...,  0.42740399,\n",
              "          0.41439342,  0.41412693],\n",
              "        [ 0.2194567 ,  0.13964303,  0.13595986, ...,  0.50349951,\n",
              "          0.44149935,  0.43563011],\n",
              "        [ 0.52221513,  0.41538206,  0.45405716, ...,  0.84061372,\n",
              "          0.76966971,  0.80492389],\n",
              "        [-0.38395184, -0.38735166, -0.45220405, ..., -0.15547469,\n",
              "         -0.17846075, -0.20321599],\n",
              "        [-0.17813811, -0.22383532, -0.1785505 , ...,  0.25661016,\n",
              "          0.16179471,  0.21348226]],\n",
              "\n",
              "       [[ 0.4540883 ,  0.46853596,  0.46586841, ...,  0.38436917,\n",
              "          0.39441386,  0.39366874],\n",
              "        [ 0.38922316,  0.40024418,  0.37380579, ...,  0.43341819,\n",
              "          0.45340902,  0.37240344],\n",
              "        [ 0.15751396,  0.16974558,  0.16511716, ...,  0.41143408,\n",
              "          0.4339388 ,  0.38584986],\n",
              "        [ 0.43235454,  0.39581606,  0.54227656, ...,  0.73533857,\n",
              "          0.8278209 ,  0.92970157],\n",
              "        [-0.47057232, -0.5356493 , -0.44554996, ..., -0.45837015,\n",
              "         -0.21979201, -0.63083887],\n",
              "        [-0.09541078, -0.55463558, -0.41003433, ...,  0.22224435,\n",
              "          0.28202656,  0.20660853]],\n",
              "\n",
              "       [[ 0.40046358,  0.39131492,  0.4231053 , ...,  0.31542367,\n",
              "          0.30583015,  0.30689392],\n",
              "        [ 0.31353337,  0.35939834,  0.39048085, ...,  0.37079248,\n",
              "          0.34838179,  0.32802367],\n",
              "        [ 0.28120154,  0.28019512,  0.28235346, ...,  0.59895134,\n",
              "          0.6051448 ,  0.58142608],\n",
              "        [ 0.51119608,  0.43256   ,  0.40535307, ...,  0.8865369 ,\n",
              "          0.86587721,  0.8706671 ],\n",
              "        [-0.40873593, -0.43950924, -0.45358857, ..., -0.17658219,\n",
              "         -0.19639042, -0.24204727],\n",
              "        [-0.30314046, -0.47210193, -0.40909085, ..., -0.00294939,\n",
              "         -0.01752386, -0.0261501 ]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 0.42645031,  0.42219424,  0.41669637, ...,  0.35376024,\n",
              "          0.34996459,  0.35232979],\n",
              "        [ 0.34910628,  0.3557384 ,  0.36022255, ...,  0.41067013,\n",
              "          0.41078925,  0.41073602],\n",
              "        [ 0.24711792,  0.25015557,  0.25022659, ...,  0.54345936,\n",
              "          0.54641563,  0.54374152],\n",
              "        [ 0.55489218,  0.54222017,  0.54272115, ...,  0.83402228,\n",
              "          0.83484739,  0.83522826],\n",
              "        [-0.37929547, -0.35301551, -0.39301932, ..., -0.16734998,\n",
              "         -0.14625142, -0.1754093 ],\n",
              "        [-0.19786355, -0.17092121, -0.17581455, ...,  0.1630057 ,\n",
              "          0.17589413,  0.17605004]],\n",
              "\n",
              "       [[ 0.4912037 ,  0.49313623,  0.48814815, ...,  0.41916737,\n",
              "          0.42017412,  0.4216938 ],\n",
              "        [ 0.42034498,  0.41882569,  0.41691756, ...,  0.46843845,\n",
              "          0.45991787,  0.4517923 ],\n",
              "        [ 0.12601115,  0.15531234,  0.17145601, ...,  0.42445728,\n",
              "          0.43774137,  0.47608137],\n",
              "        [ 0.53036183,  0.42807087,  0.47857895, ...,  0.84186035,\n",
              "          0.78463817,  0.80949044],\n",
              "        [-0.32037112, -0.34775042, -0.34031934, ..., -0.12510928,\n",
              "         -0.16310829, -0.16897482],\n",
              "        [-0.19512436, -0.13677415, -0.20170197, ...,  0.2525183 ,\n",
              "          0.19413811,  0.24267113]],\n",
              "\n",
              "       [[ 0.49379402,  0.49444228,  0.49889559, ...,  0.40431261,\n",
              "          0.40488309,  0.40226346],\n",
              "        [ 0.4092218 ,  0.41504175,  0.41780323, ...,  0.46549258,\n",
              "          0.47851556,  0.48092693],\n",
              "        [ 0.19811314,  0.17408006,  0.23009348, ...,  0.53245091,\n",
              "          0.48964956,  0.56968272],\n",
              "        [ 0.57573336,  0.5060215 ,  0.50277686, ...,  0.88817918,\n",
              "          0.87062991,  0.87335479],\n",
              "        [-0.48746079, -0.41942352, -0.37562895, ..., -0.22516759,\n",
              "         -0.16274184, -0.15341888],\n",
              "        [-0.14664087, -0.15010014, -0.14120756, ...,  0.32423443,\n",
              "          0.19089164,  0.13804731]]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_test.shape"
      ],
      "metadata": {
        "id": "kWNyJPcyHgX_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d04a8d5-bd76-449f-e576-3c607031b4c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3044, 6, 39), (762, 6, 39))"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0].shape"
      ],
      "metadata": {
        "id": "pRoaKE-CdwKQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80e3b261-fa19-43b6-e4a6-cc46c06c6481"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 39)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test[0].shape"
      ],
      "metadata": {
        "id": "IlOyx_Jyd_Bk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adf5cebf-030f-4399-dfa9-fcff94b860ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 39)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0].shape, X_test[0].shape"
      ],
      "metadata": {
        "id": "zFK3Ds-JHrzC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4923411a-50b2-48c1-a6db-8ba098b4237b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((6, 39), (6, 39))"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.reshape(3044, 6, 39, 1)\n",
        "X_test = X_test.reshape(762, 6, 39, 1)"
      ],
      "metadata": {
        "id": "Ab6yOdz9H5DV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0].shape, X_test[0].shape\n"
      ],
      "metadata": {
        "id": "soVmYWwzIJ42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "977d3d6d-5f0c-4030-9339-29a8c137e8ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((6, 39, 1), (6, 39, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WeZ2bJv0oTgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2D CNN MODEL"
      ],
      "metadata": {
        "id": "qunl7GSDIUOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(16, (2, 2), activation = 'relu', input_shape = X_train[0].shape))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Conv2D(32, (2, 2), activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(64, activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(6, activation='softmax'))"
      ],
      "metadata": {
        "id": "U9l6iACpInNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential() \n",
        "# model.add(Flatten(input_shape=X_train.shape[1:])) \n",
        "model.add(Flatten(input_shape=(3,39))) \n",
        "model.add(Dense(100, activation='relu')) \n",
        "model.add(Dropout(0.5)) \n",
        "model.add(Dense(50, activation='relu')) \n",
        "model.add(Dropout(0.3)) \n",
        "model.add(Dense(6, activation='softmax'))"
      ],
      "metadata": {
        "id": "B_QkX1UZJSIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=Adam(learning_rate = 0.001), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "O5Dv2NG2I8Bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# history = model.fit(X_train, y_train, epochs = 100, validation_data= (X_test, y_test), verbose=1)\n",
        "history = model.fit(X_train, y_train, epochs = 100, validation_split=0.2, verbose=1)"
      ],
      "metadata": {
        "id": "F-oEwl9JJ4kx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14fc401c-57ad-4458-9260-fadc4910c562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "77/77 [==============================] - 2s 19ms/step - loss: 1.4441 - accuracy: 0.4090 - val_loss: 0.8392 - val_accuracy: 0.8719\n",
            "Epoch 2/100\n",
            "77/77 [==============================] - 1s 15ms/step - loss: 0.7211 - accuracy: 0.7257 - val_loss: 0.4522 - val_accuracy: 0.7471\n",
            "Epoch 3/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.4620 - accuracy: 0.8168 - val_loss: 0.2310 - val_accuracy: 0.9589\n",
            "Epoch 4/100\n",
            "77/77 [==============================] - 1s 15ms/step - loss: 0.3391 - accuracy: 0.8706 - val_loss: 0.1482 - val_accuracy: 0.9721\n",
            "Epoch 5/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.2739 - accuracy: 0.9006 - val_loss: 0.0922 - val_accuracy: 0.9934\n",
            "Epoch 6/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.2383 - accuracy: 0.9162 - val_loss: 0.0853 - val_accuracy: 0.9918\n",
            "Epoch 7/100\n",
            "77/77 [==============================] - 1s 17ms/step - loss: 0.2234 - accuracy: 0.9170 - val_loss: 0.0746 - val_accuracy: 0.9819\n",
            "Epoch 8/100\n",
            "77/77 [==============================] - 1s 17ms/step - loss: 0.1659 - accuracy: 0.9437 - val_loss: 0.0389 - val_accuracy: 0.9934\n",
            "Epoch 9/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.1614 - accuracy: 0.9396 - val_loss: 0.0335 - val_accuracy: 0.9967\n",
            "Epoch 10/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.1487 - accuracy: 0.9474 - val_loss: 0.0365 - val_accuracy: 0.9918\n",
            "Epoch 11/100\n",
            "77/77 [==============================] - 2s 25ms/step - loss: 0.1493 - accuracy: 0.9425 - val_loss: 0.0676 - val_accuracy: 0.9852\n",
            "Epoch 12/100\n",
            "77/77 [==============================] - 2s 25ms/step - loss: 0.1359 - accuracy: 0.9573 - val_loss: 0.0259 - val_accuracy: 0.9951\n",
            "Epoch 13/100\n",
            "77/77 [==============================] - 1s 15ms/step - loss: 0.1056 - accuracy: 0.9688 - val_loss: 0.0173 - val_accuracy: 0.9967\n",
            "Epoch 14/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.1262 - accuracy: 0.9589 - val_loss: 0.0163 - val_accuracy: 0.9967\n",
            "Epoch 15/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.1175 - accuracy: 0.9573 - val_loss: 0.0269 - val_accuracy: 0.9918\n",
            "Epoch 16/100\n",
            "77/77 [==============================] - 1s 17ms/step - loss: 0.1231 - accuracy: 0.9589 - val_loss: 0.0319 - val_accuracy: 0.9918\n",
            "Epoch 17/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.1110 - accuracy: 0.9663 - val_loss: 0.0173 - val_accuracy: 0.9967\n",
            "Epoch 18/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0906 - accuracy: 0.9725 - val_loss: 0.0177 - val_accuracy: 0.9934\n",
            "Epoch 19/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0912 - accuracy: 0.9721 - val_loss: 0.0249 - val_accuracy: 0.9934\n",
            "Epoch 20/100\n",
            "77/77 [==============================] - 1s 15ms/step - loss: 0.0818 - accuracy: 0.9729 - val_loss: 0.0186 - val_accuracy: 0.9934\n",
            "Epoch 21/100\n",
            "77/77 [==============================] - 1s 17ms/step - loss: 0.1074 - accuracy: 0.9651 - val_loss: 0.0310 - val_accuracy: 0.9869\n",
            "Epoch 22/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.1085 - accuracy: 0.9651 - val_loss: 0.0130 - val_accuracy: 0.9984\n",
            "Epoch 23/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0842 - accuracy: 0.9725 - val_loss: 0.0124 - val_accuracy: 0.9967\n",
            "Epoch 24/100\n",
            "77/77 [==============================] - 1s 17ms/step - loss: 0.0808 - accuracy: 0.9733 - val_loss: 0.0073 - val_accuracy: 0.9984\n",
            "Epoch 25/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.1013 - accuracy: 0.9651 - val_loss: 0.0135 - val_accuracy: 0.9967\n",
            "Epoch 26/100\n",
            "77/77 [==============================] - 1s 17ms/step - loss: 0.0785 - accuracy: 0.9721 - val_loss: 0.0108 - val_accuracy: 0.9967\n",
            "Epoch 27/100\n",
            "77/77 [==============================] - 1s 17ms/step - loss: 0.0909 - accuracy: 0.9737 - val_loss: 0.0072 - val_accuracy: 0.9967\n",
            "Epoch 28/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0770 - accuracy: 0.9770 - val_loss: 0.0112 - val_accuracy: 0.9967\n",
            "Epoch 29/100\n",
            "77/77 [==============================] - 1s 15ms/step - loss: 0.0764 - accuracy: 0.9704 - val_loss: 0.0206 - val_accuracy: 0.9934\n",
            "Epoch 30/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0771 - accuracy: 0.9737 - val_loss: 0.0098 - val_accuracy: 0.9967\n",
            "Epoch 31/100\n",
            "77/77 [==============================] - 1s 17ms/step - loss: 0.0661 - accuracy: 0.9762 - val_loss: 0.0205 - val_accuracy: 0.9934\n",
            "Epoch 32/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0721 - accuracy: 0.9758 - val_loss: 0.0104 - val_accuracy: 0.9967\n",
            "Epoch 33/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0677 - accuracy: 0.9795 - val_loss: 0.0144 - val_accuracy: 0.9934\n",
            "Epoch 34/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0604 - accuracy: 0.9799 - val_loss: 0.0053 - val_accuracy: 0.9984\n",
            "Epoch 35/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0670 - accuracy: 0.9737 - val_loss: 0.0043 - val_accuracy: 0.9984\n",
            "Epoch 36/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0461 - accuracy: 0.9836 - val_loss: 0.0027 - val_accuracy: 0.9984\n",
            "Epoch 37/100\n",
            "77/77 [==============================] - 1s 17ms/step - loss: 0.0627 - accuracy: 0.9782 - val_loss: 0.0057 - val_accuracy: 0.9984\n",
            "Epoch 38/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0808 - accuracy: 0.9704 - val_loss: 0.0130 - val_accuracy: 0.9951\n",
            "Epoch 39/100\n",
            "77/77 [==============================] - 1s 17ms/step - loss: 0.0564 - accuracy: 0.9848 - val_loss: 0.0095 - val_accuracy: 0.9967\n",
            "Epoch 40/100\n",
            "77/77 [==============================] - 1s 17ms/step - loss: 0.0538 - accuracy: 0.9791 - val_loss: 0.0048 - val_accuracy: 0.9967\n",
            "Epoch 41/100\n",
            "77/77 [==============================] - 2s 26ms/step - loss: 0.0568 - accuracy: 0.9778 - val_loss: 0.0141 - val_accuracy: 0.9951\n",
            "Epoch 42/100\n",
            "77/77 [==============================] - 2s 31ms/step - loss: 0.0599 - accuracy: 0.9795 - val_loss: 0.0132 - val_accuracy: 0.9951\n",
            "Epoch 43/100\n",
            "77/77 [==============================] - 1s 17ms/step - loss: 0.0461 - accuracy: 0.9844 - val_loss: 0.0029 - val_accuracy: 0.9984\n",
            "Epoch 44/100\n",
            "77/77 [==============================] - 1s 17ms/step - loss: 0.0508 - accuracy: 0.9782 - val_loss: 0.0076 - val_accuracy: 0.9967\n",
            "Epoch 45/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0609 - accuracy: 0.9778 - val_loss: 0.0084 - val_accuracy: 0.9967\n",
            "Epoch 46/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0431 - accuracy: 0.9848 - val_loss: 0.0076 - val_accuracy: 0.9967\n",
            "Epoch 47/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0423 - accuracy: 0.9840 - val_loss: 0.0034 - val_accuracy: 0.9984\n",
            "Epoch 48/100\n",
            "77/77 [==============================] - 1s 17ms/step - loss: 0.0598 - accuracy: 0.9745 - val_loss: 0.0076 - val_accuracy: 0.9967\n",
            "Epoch 49/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0562 - accuracy: 0.9799 - val_loss: 0.0025 - val_accuracy: 0.9984\n",
            "Epoch 50/100\n",
            "77/77 [==============================] - 1s 17ms/step - loss: 0.0430 - accuracy: 0.9864 - val_loss: 0.0044 - val_accuracy: 0.9967\n",
            "Epoch 51/100\n",
            "77/77 [==============================] - 1s 17ms/step - loss: 0.0548 - accuracy: 0.9795 - val_loss: 0.0054 - val_accuracy: 0.9984\n",
            "Epoch 52/100\n",
            "77/77 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9791 - val_loss: 0.0076 - val_accuracy: 0.9984\n",
            "Epoch 53/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0561 - accuracy: 0.9786 - val_loss: 0.0066 - val_accuracy: 0.9967\n",
            "Epoch 54/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0404 - accuracy: 0.9852 - val_loss: 0.0035 - val_accuracy: 0.9984\n",
            "Epoch 55/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0442 - accuracy: 0.9828 - val_loss: 0.0024 - val_accuracy: 0.9984\n",
            "Epoch 56/100\n",
            "77/77 [==============================] - 2s 24ms/step - loss: 0.0441 - accuracy: 0.9864 - val_loss: 0.0082 - val_accuracy: 0.9951\n",
            "Epoch 57/100\n",
            "77/77 [==============================] - 2s 23ms/step - loss: 0.0364 - accuracy: 0.9889 - val_loss: 0.0048 - val_accuracy: 0.9984\n",
            "Epoch 58/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0537 - accuracy: 0.9791 - val_loss: 0.0032 - val_accuracy: 0.9984\n",
            "Epoch 59/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0380 - accuracy: 0.9869 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "77/77 [==============================] - 1s 17ms/step - loss: 0.0455 - accuracy: 0.9844 - val_loss: 0.0052 - val_accuracy: 0.9984\n",
            "Epoch 61/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0478 - accuracy: 0.9819 - val_loss: 0.0061 - val_accuracy: 0.9984\n",
            "Epoch 62/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0667 - accuracy: 0.9737 - val_loss: 0.0080 - val_accuracy: 0.9967\n",
            "Epoch 63/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0434 - accuracy: 0.9860 - val_loss: 0.0040 - val_accuracy: 0.9984\n",
            "Epoch 64/100\n",
            "77/77 [==============================] - 1s 17ms/step - loss: 0.0465 - accuracy: 0.9832 - val_loss: 0.0021 - val_accuracy: 0.9984\n",
            "Epoch 65/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0392 - accuracy: 0.9844 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "77/77 [==============================] - 1s 17ms/step - loss: 0.0402 - accuracy: 0.9844 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0466 - accuracy: 0.9856 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0508 - accuracy: 0.9819 - val_loss: 0.0053 - val_accuracy: 0.9967\n",
            "Epoch 69/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0506 - accuracy: 0.9815 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0420 - accuracy: 0.9823 - val_loss: 0.0047 - val_accuracy: 0.9967\n",
            "Epoch 71/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0614 - accuracy: 0.9786 - val_loss: 0.0096 - val_accuracy: 0.9967\n",
            "Epoch 72/100\n",
            "77/77 [==============================] - 1s 18ms/step - loss: 0.0457 - accuracy: 0.9815 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0415 - accuracy: 0.9840 - val_loss: 0.0104 - val_accuracy: 0.9951\n",
            "Epoch 74/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0470 - accuracy: 0.9811 - val_loss: 0.0084 - val_accuracy: 0.9967\n",
            "Epoch 75/100\n",
            "77/77 [==============================] - 1s 17ms/step - loss: 0.0435 - accuracy: 0.9819 - val_loss: 0.0079 - val_accuracy: 0.9967\n",
            "Epoch 76/100\n",
            "77/77 [==============================] - 1s 17ms/step - loss: 0.0464 - accuracy: 0.9856 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "77/77 [==============================] - 1s 17ms/step - loss: 0.0423 - accuracy: 0.9832 - val_loss: 0.0052 - val_accuracy: 0.9984\n",
            "Epoch 78/100\n",
            "77/77 [==============================] - 1s 17ms/step - loss: 0.0373 - accuracy: 0.9856 - val_loss: 0.0206 - val_accuracy: 0.9918\n",
            "Epoch 79/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0582 - accuracy: 0.9758 - val_loss: 0.0049 - val_accuracy: 0.9967\n",
            "Epoch 80/100\n",
            "77/77 [==============================] - 1s 17ms/step - loss: 0.0367 - accuracy: 0.9856 - val_loss: 0.0015 - val_accuracy: 0.9984\n",
            "Epoch 81/100\n",
            "77/77 [==============================] - 1s 17ms/step - loss: 0.0337 - accuracy: 0.9877 - val_loss: 0.0051 - val_accuracy: 0.9984\n",
            "Epoch 82/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0398 - accuracy: 0.9848 - val_loss: 0.0022 - val_accuracy: 0.9984\n",
            "Epoch 83/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0441 - accuracy: 0.9828 - val_loss: 0.0036 - val_accuracy: 0.9984\n",
            "Epoch 84/100\n",
            "77/77 [==============================] - 1s 17ms/step - loss: 0.0411 - accuracy: 0.9832 - val_loss: 0.0037 - val_accuracy: 0.9984\n",
            "Epoch 85/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0351 - accuracy: 0.9877 - val_loss: 0.0019 - val_accuracy: 0.9984\n",
            "Epoch 86/100\n",
            "77/77 [==============================] - 1s 17ms/step - loss: 0.0478 - accuracy: 0.9791 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0332 - accuracy: 0.9864 - val_loss: 0.0023 - val_accuracy: 0.9984\n",
            "Epoch 88/100\n",
            "77/77 [==============================] - 1s 17ms/step - loss: 0.0336 - accuracy: 0.9856 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "77/77 [==============================] - 1s 17ms/step - loss: 0.0318 - accuracy: 0.9864 - val_loss: 0.0028 - val_accuracy: 0.9984\n",
            "Epoch 90/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0301 - accuracy: 0.9885 - val_loss: 0.0035 - val_accuracy: 0.9967\n",
            "Epoch 91/100\n",
            "77/77 [==============================] - 1s 17ms/step - loss: 0.0438 - accuracy: 0.9836 - val_loss: 8.6047e-04 - val_accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0414 - accuracy: 0.9848 - val_loss: 0.0031 - val_accuracy: 0.9984\n",
            "Epoch 93/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0537 - accuracy: 0.9791 - val_loss: 0.0019 - val_accuracy: 0.9984\n",
            "Epoch 94/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0353 - accuracy: 0.9844 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0363 - accuracy: 0.9881 - val_loss: 0.0020 - val_accuracy: 0.9984\n",
            "Epoch 96/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0252 - accuracy: 0.9901 - val_loss: 0.0086 - val_accuracy: 0.9967\n",
            "Epoch 97/100\n",
            "77/77 [==============================] - 1s 18ms/step - loss: 0.0416 - accuracy: 0.9860 - val_loss: 0.0095 - val_accuracy: 0.9951\n",
            "Epoch 98/100\n",
            "77/77 [==============================] - 1s 16ms/step - loss: 0.0312 - accuracy: 0.9873 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "77/77 [==============================] - 1s 18ms/step - loss: 0.0449 - accuracy: 0.9832 - val_loss: 0.0028 - val_accuracy: 0.9984\n",
            "Epoch 100/100\n",
            "77/77 [==============================] - 1s 17ms/step - loss: 0.0302 - accuracy: 0.9877 - val_loss: 7.2375e-04 - val_accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot loss\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.subplot(211)\n",
        "plt.title('Loss Plot ')\n",
        "# plt.plot(my_history['loss'], label='train')\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "# plt.plot(my_history['val_loss'], label='val')\n",
        "plt.plot(history.history['val_loss'], label='val')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# plot validation accuracy\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.subplot(2,1,1)\n",
        "# plt.plot(my_history['accuracy'])\n",
        "plt.plot(history.history['accuracy'])\n",
        "# plt.plot(my_history['val_accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='lower right')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636
        },
        "id": "9Idm_oPSWq0A",
        "outputId": "585ef319-3864-494a-ea61-6989cdd3d006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fe7015f3c90>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def model_builder_hp(hp):\n",
        "  model = keras.Sequential()\n",
        "  # model.add(keras.layers.InputLayer(Dropout(0.4,input_shape=(13,))))\n",
        "  # model.add(keras.layers.InputLayer(input_shape=(117)))\n",
        "  model.add(Flatten(input_shape=X_train.shape[1:])) \n",
        "  # model.add(keras.layers.InputLayer.input_spec(Dropout(0.4, input_shape=(13,))))\n",
        "  # model.add(keras.layers.InputLayer(input_shape=(13)))\n",
        "  model.add(keras.layers.BatchNormalization())\n",
        "  \n",
        "  for i in range(hp.Int('num_layers', 2, 10)):\n",
        "    # model.add(keras.layers.Dense(units=hp.Int('units_' + str(i),min_value=4,max_value=64,step=4),kernel_constraint=maxnorm(3), activation=hp.Choice('act_'+str(i), values=['relu', 'tanh', 'sigmoid'], default='relu')))\n",
        "    model.add(keras.layers.Dense(units=hp.Int('units_' + str(i),min_value=13,max_value=210,step=4), activation=hp.Choice('act_'+str(i), values=['relu', 'tanh', 'sigmoid'], default='relu')))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "    model.add(keras.layers.Dropout(rate=hp.Float('dropout_1',min_value=0.0,max_value=0.5,default=0.25,step=0.05)))\n",
        "\n",
        "  #for output layer\n",
        "  model.add(keras.layers.Dense(6, activation='softmax')) \n",
        "  # model.add(Activation(tf.nn.sigmoid))\n",
        "\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "  # Select optimizer    \n",
        "  optimizer=hp.Choice('optimizer', values=['adam', 'adagrad', 'SGD', 'RMSprop'])\n",
        "\n",
        "  # Conditional for each optimizer\n",
        "  if optimizer == 'adam':\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate)\n",
        "  elif optimizer == 'adagrad':\n",
        "    optimizer=keras.optimizers.Adagrad(learning_rate=hp_learning_rate)\n",
        "  elif optimizer == 'SGD':\n",
        "    optimizer=keras.optimizers.SGD(learning_rate=hp_learning_rate)\n",
        "  elif optimizer == 'RMSprop':\n",
        "    optimizer=keras.optimizers.RMSprop(learning_rate=hp_learning_rate)\n",
        "\n",
        "  # Now compile your model with previous param\n",
        "  model.compile(optimizer='adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    reduction=\"auto\", name=\"sparse_categorical_crossentropy\"\n",
        "),metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "AzjKA8I2LW35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil;\n",
        "# shutil.rmtree('tuner_Dir') \n",
        "tuner = kt.Hyperband(model_builder_hp,\n",
        "                     objective='val_accuracy',\n",
        "                     max_epochs=10,\n",
        "                     factor=3,\n",
        "                     directory='tuner_Dir',\n",
        "                     project_name='pose_Cnn_New')"
      ],
      "metadata": {
        "id": "_zPWPV0VLklB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
      ],
      "metadata": {
        "id": "JDDBIXg7N2BP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(X, y, epochs=100, validation_split=0.2, callbacks=[stop_early])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AP54pgFN643",
        "outputId": "e06e4ef5-7a4c-4b6c-9736-037db61b2bf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 30 Complete [00h 00m 09s]\n",
            "val_accuracy: 0.8530183434486389\n",
            "\n",
            "Best val_accuracy So Far: 1.0\n",
            "Total elapsed time: 00h 03m 04s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.results_summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5cnHb6JOfSn",
        "outputId": "4cc9841a-8c96-4fb3-d876-92c4eab47b86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results summary\n",
            "Results in tuner_Dir/pose_Cnn_New\n",
            "Showing 10 best trials\n",
            "<keras_tuner.engine.objective.Objective object at 0x7fe7015de310>\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 4\n",
            "units_0: 125\n",
            "act_0: sigmoid\n",
            "dropout_1: 0.30000000000000004\n",
            "units_1: 85\n",
            "act_1: tanh\n",
            "learning_rate: 0.01\n",
            "optimizer: adagrad\n",
            "units_2: 157\n",
            "act_2: sigmoid\n",
            "units_3: 173\n",
            "act_3: sigmoid\n",
            "units_4: 129\n",
            "act_4: relu\n",
            "units_5: 157\n",
            "act_5: relu\n",
            "units_6: 197\n",
            "act_6: sigmoid\n",
            "units_7: 189\n",
            "act_7: sigmoid\n",
            "units_8: 117\n",
            "act_8: tanh\n",
            "tuner/epochs: 2\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 2\n",
            "tuner/round: 0\n",
            "Score: 1.0\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 5\n",
            "units_0: 193\n",
            "act_0: relu\n",
            "dropout_1: 0.2\n",
            "units_1: 93\n",
            "act_1: relu\n",
            "learning_rate: 0.001\n",
            "optimizer: RMSprop\n",
            "units_2: 41\n",
            "act_2: sigmoid\n",
            "units_3: 113\n",
            "act_3: sigmoid\n",
            "units_4: 57\n",
            "act_4: sigmoid\n",
            "units_5: 29\n",
            "act_5: relu\n",
            "units_6: 145\n",
            "act_6: relu\n",
            "units_7: 121\n",
            "act_7: sigmoid\n",
            "units_8: 85\n",
            "act_8: relu\n",
            "tuner/epochs: 10\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 0\n",
            "tuner/round: 0\n",
            "Score: 0.9763779640197754\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 7\n",
            "units_0: 141\n",
            "act_0: relu\n",
            "dropout_1: 0.05\n",
            "units_1: 89\n",
            "act_1: tanh\n",
            "learning_rate: 0.01\n",
            "optimizer: RMSprop\n",
            "units_2: 129\n",
            "act_2: sigmoid\n",
            "units_3: 93\n",
            "act_3: tanh\n",
            "units_4: 61\n",
            "act_4: relu\n",
            "units_5: 201\n",
            "act_5: sigmoid\n",
            "units_6: 125\n",
            "act_6: relu\n",
            "units_7: 181\n",
            "act_7: sigmoid\n",
            "units_8: 129\n",
            "act_8: tanh\n",
            "tuner/epochs: 10\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 0\n",
            "tuner/round: 0\n",
            "Score: 0.9501312375068665\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 2\n",
            "units_0: 49\n",
            "act_0: tanh\n",
            "dropout_1: 0.2\n",
            "units_1: 197\n",
            "act_1: tanh\n",
            "learning_rate: 0.01\n",
            "optimizer: RMSprop\n",
            "units_2: 169\n",
            "act_2: tanh\n",
            "units_3: 21\n",
            "act_3: relu\n",
            "units_4: 81\n",
            "act_4: relu\n",
            "units_5: 109\n",
            "act_5: sigmoid\n",
            "units_6: 153\n",
            "act_6: relu\n",
            "units_7: 89\n",
            "act_7: sigmoid\n",
            "units_8: 121\n",
            "act_8: tanh\n",
            "tuner/epochs: 10\n",
            "tuner/initial_epoch: 4\n",
            "tuner/bracket: 2\n",
            "tuner/round: 2\n",
            "tuner/trial_id: 0013\n",
            "Score: 0.9488189220428467\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 4\n",
            "units_0: 125\n",
            "act_0: sigmoid\n",
            "dropout_1: 0.30000000000000004\n",
            "units_1: 85\n",
            "act_1: tanh\n",
            "learning_rate: 0.01\n",
            "optimizer: adagrad\n",
            "units_2: 157\n",
            "act_2: sigmoid\n",
            "units_3: 173\n",
            "act_3: sigmoid\n",
            "units_4: 129\n",
            "act_4: relu\n",
            "units_5: 157\n",
            "act_5: relu\n",
            "units_6: 197\n",
            "act_6: sigmoid\n",
            "units_7: 189\n",
            "act_7: sigmoid\n",
            "units_8: 117\n",
            "act_8: tanh\n",
            "tuner/epochs: 10\n",
            "tuner/initial_epoch: 4\n",
            "tuner/bracket: 2\n",
            "tuner/round: 2\n",
            "tuner/trial_id: 0012\n",
            "Score: 0.887139081954956\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 4\n",
            "units_0: 21\n",
            "act_0: sigmoid\n",
            "dropout_1: 0.1\n",
            "units_1: 57\n",
            "act_1: sigmoid\n",
            "learning_rate: 0.0001\n",
            "optimizer: RMSprop\n",
            "units_2: 113\n",
            "act_2: tanh\n",
            "units_3: 73\n",
            "act_3: sigmoid\n",
            "units_4: 109\n",
            "act_4: sigmoid\n",
            "units_5: 145\n",
            "act_5: sigmoid\n",
            "units_6: 169\n",
            "act_6: tanh\n",
            "units_7: 93\n",
            "act_7: sigmoid\n",
            "units_8: 193\n",
            "act_8: tanh\n",
            "tuner/epochs: 4\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 1\n",
            "tuner/round: 0\n",
            "Score: 0.8608924150466919\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 6\n",
            "units_0: 29\n",
            "act_0: sigmoid\n",
            "dropout_1: 0.0\n",
            "units_1: 209\n",
            "act_1: relu\n",
            "learning_rate: 0.0001\n",
            "optimizer: RMSprop\n",
            "units_2: 141\n",
            "act_2: relu\n",
            "units_3: 21\n",
            "act_3: tanh\n",
            "units_4: 173\n",
            "act_4: sigmoid\n",
            "units_5: 97\n",
            "act_5: tanh\n",
            "units_6: 21\n",
            "act_6: sigmoid\n",
            "units_7: 149\n",
            "act_7: relu\n",
            "units_8: 153\n",
            "act_8: sigmoid\n",
            "tuner/epochs: 10\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 0\n",
            "tuner/round: 0\n",
            "Score: 0.8530183434486389\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 7\n",
            "units_0: 193\n",
            "act_0: tanh\n",
            "dropout_1: 0.30000000000000004\n",
            "units_1: 65\n",
            "act_1: sigmoid\n",
            "learning_rate: 0.001\n",
            "optimizer: adagrad\n",
            "units_2: 53\n",
            "act_2: relu\n",
            "units_3: 45\n",
            "act_3: sigmoid\n",
            "units_4: 125\n",
            "act_4: tanh\n",
            "units_5: 133\n",
            "act_5: tanh\n",
            "units_6: 121\n",
            "act_6: sigmoid\n",
            "units_7: 133\n",
            "act_7: relu\n",
            "units_8: 125\n",
            "act_8: relu\n",
            "tuner/epochs: 10\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 0\n",
            "tuner/round: 0\n",
            "Score: 0.7847769260406494\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 2\n",
            "units_0: 49\n",
            "act_0: tanh\n",
            "dropout_1: 0.2\n",
            "units_1: 197\n",
            "act_1: tanh\n",
            "learning_rate: 0.01\n",
            "optimizer: RMSprop\n",
            "units_2: 169\n",
            "act_2: tanh\n",
            "units_3: 21\n",
            "act_3: relu\n",
            "units_4: 81\n",
            "act_4: relu\n",
            "units_5: 109\n",
            "act_5: sigmoid\n",
            "units_6: 153\n",
            "act_6: relu\n",
            "units_7: 89\n",
            "act_7: sigmoid\n",
            "units_8: 121\n",
            "act_8: tanh\n",
            "tuner/epochs: 4\n",
            "tuner/initial_epoch: 2\n",
            "tuner/bracket: 2\n",
            "tuner/round: 1\n",
            "tuner/trial_id: 0009\n",
            "Score: 0.7650918364524841\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 4\n",
            "units_0: 21\n",
            "act_0: sigmoid\n",
            "dropout_1: 0.1\n",
            "units_1: 57\n",
            "act_1: sigmoid\n",
            "learning_rate: 0.0001\n",
            "optimizer: RMSprop\n",
            "units_2: 113\n",
            "act_2: tanh\n",
            "units_3: 73\n",
            "act_3: sigmoid\n",
            "units_4: 109\n",
            "act_4: sigmoid\n",
            "units_5: 145\n",
            "act_5: sigmoid\n",
            "units_6: 169\n",
            "act_6: tanh\n",
            "units_7: 93\n",
            "act_7: sigmoid\n",
            "units_8: 193\n",
            "act_8: tanh\n",
            "tuner/epochs: 10\n",
            "tuner/initial_epoch: 4\n",
            "tuner/bracket: 1\n",
            "tuner/round: 1\n",
            "tuner/trial_id: 0022\n",
            "Score: 0.6863517165184021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSuIYy0WOiSw",
        "outputId": "58b7bf6a-912a-43e4-d7a3-067306cfce5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_1 (Flatten)         (None, 234)               0         \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 234)              936       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 125)               29375     \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 125)              500       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 125)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 85)                10710     \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 85)               340       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 85)                0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 157)               13502     \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 157)              628       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 157)               0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 173)               27334     \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 173)              692       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 173)               0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 6)                 1044      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 85,061\n",
            "Trainable params: 83,513\n",
            "Non-trainable params: 1,548\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X, y, epochs=500, validation_split=0.3)\n",
        "# history = model.fit(X, y, epochs=500, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJvOUfB_Omre",
        "outputId": "30389a9d-9c04-4be4-98e5-971d1024bd64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "84/84 [==============================] - 1s 16ms/step - loss: 0.0386 - accuracy: 0.9959 - val_loss: 0.0712 - val_accuracy: 0.9842\n",
            "Epoch 2/500\n",
            "84/84 [==============================] - 1s 11ms/step - loss: 0.0101 - accuracy: 0.9985 - val_loss: 0.0276 - val_accuracy: 0.9991\n",
            "Epoch 3/500\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0034 - accuracy: 0.9996 - val_loss: 0.0226 - val_accuracy: 0.9991\n",
            "Epoch 4/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.5409e-04 - accuracy: 1.0000 - val_loss: 0.0179 - val_accuracy: 0.9991\n",
            "Epoch 5/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0017 - accuracy: 0.9992 - val_loss: 0.0123 - val_accuracy: 1.0000\n",
            "Epoch 6/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0018 - accuracy: 0.9992 - val_loss: 0.0137 - val_accuracy: 1.0000\n",
            "Epoch 7/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0043 - accuracy: 0.9992 - val_loss: 0.2386 - val_accuracy: 0.9282\n",
            "Epoch 8/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0116 - accuracy: 0.9977 - val_loss: 0.3992 - val_accuracy: 0.8214\n",
            "Epoch 9/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 8.2401e-04 - accuracy: 0.9996 - val_loss: 1.7879 - val_accuracy: 0.3695\n",
            "Epoch 10/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 4.7640e-04 - accuracy: 1.0000 - val_loss: 1.6621 - val_accuracy: 0.3879\n",
            "Epoch 11/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0077 - accuracy: 0.9989 - val_loss: 0.9477 - val_accuracy: 0.5709\n",
            "Epoch 12/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0023 - accuracy: 0.9985 - val_loss: 1.4597 - val_accuracy: 0.4203\n",
            "Epoch 13/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 2.2504e-04 - accuracy: 1.0000 - val_loss: 1.4764 - val_accuracy: 0.4177\n",
            "Epoch 14/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0048 - accuracy: 0.9996 - val_loss: 1.4315 - val_accuracy: 0.4098\n",
            "Epoch 15/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 1.0693 - val_accuracy: 0.4562\n",
            "Epoch 16/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.5631 - val_accuracy: 0.7005\n",
            "Epoch 17/500\n",
            "84/84 [==============================] - 1s 6ms/step - loss: 0.0215 - accuracy: 0.9970 - val_loss: 0.2797 - val_accuracy: 0.9282\n",
            "Epoch 18/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0027 - accuracy: 0.9996 - val_loss: 0.3523 - val_accuracy: 0.8511\n",
            "Epoch 19/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.5041 - val_accuracy: 0.7014\n",
            "Epoch 20/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0035 - accuracy: 0.9989 - val_loss: 0.4046 - val_accuracy: 0.7960\n",
            "Epoch 21/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 4.7896e-04 - accuracy: 0.9996 - val_loss: 0.4472 - val_accuracy: 0.7574\n",
            "Epoch 22/500\n",
            "84/84 [==============================] - 1s 6ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.4860 - val_accuracy: 0.7207\n",
            "Epoch 23/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 8.2559e-04 - accuracy: 0.9996 - val_loss: 0.5743 - val_accuracy: 0.6287\n",
            "Epoch 24/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 3.9759e-04 - accuracy: 1.0000 - val_loss: 0.5654 - val_accuracy: 0.6585\n",
            "Epoch 25/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 4.3486e-04 - accuracy: 1.0000 - val_loss: 0.7958 - val_accuracy: 0.5193\n",
            "Epoch 26/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 6.2032e-04 - accuracy: 0.9996 - val_loss: 0.8639 - val_accuracy: 0.4825\n",
            "Epoch 27/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 6.5571e-05 - accuracy: 1.0000 - val_loss: 0.9211 - val_accuracy: 0.4694\n",
            "Epoch 28/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0010 - accuracy: 0.9992 - val_loss: 0.9389 - val_accuracy: 0.4790\n",
            "Epoch 29/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0020 - accuracy: 0.9992 - val_loss: 1.2438 - val_accuracy: 0.4011\n",
            "Epoch 30/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 4.6039e-04 - accuracy: 1.0000 - val_loss: 1.5148 - val_accuracy: 0.3818\n",
            "Epoch 31/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0040 - accuracy: 0.9996 - val_loss: 1.5138 - val_accuracy: 0.3783\n",
            "Epoch 32/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 1.5216 - val_accuracy: 0.3792\n",
            "Epoch 33/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 4.8302e-04 - accuracy: 0.9996 - val_loss: 2.1435 - val_accuracy: 0.3494\n",
            "Epoch 34/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 4.4573e-04 - accuracy: 1.0000 - val_loss: 1.9751 - val_accuracy: 0.3581\n",
            "Epoch 35/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 5.1049e-05 - accuracy: 1.0000 - val_loss: 1.9014 - val_accuracy: 0.3625\n",
            "Epoch 36/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.1326e-04 - accuracy: 1.0000 - val_loss: 1.8280 - val_accuracy: 0.3713\n",
            "Epoch 37/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 6.0331e-05 - accuracy: 1.0000 - val_loss: 1.7414 - val_accuracy: 0.3800\n",
            "Epoch 38/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 3.4132e-05 - accuracy: 1.0000 - val_loss: 1.7383 - val_accuracy: 0.3818\n",
            "Epoch 39/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0013 - accuracy: 0.9992 - val_loss: 1.1554 - val_accuracy: 0.4273\n",
            "Epoch 40/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 2.2465e-04 - accuracy: 1.0000 - val_loss: 1.0963 - val_accuracy: 0.4370\n",
            "Epoch 41/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 6.9261e-05 - accuracy: 1.0000 - val_loss: 1.0962 - val_accuracy: 0.4291\n",
            "Epoch 42/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0034 - accuracy: 0.9989 - val_loss: 1.2628 - val_accuracy: 0.4194\n",
            "Epoch 43/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0036 - accuracy: 0.9985 - val_loss: 1.2126 - val_accuracy: 0.3783\n",
            "Epoch 44/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 5.6434e-04 - accuracy: 0.9996 - val_loss: 1.1289 - val_accuracy: 0.4510\n",
            "Epoch 45/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 2.4949e-04 - accuracy: 1.0000 - val_loss: 1.1899 - val_accuracy: 0.4229\n",
            "Epoch 46/500\n",
            "84/84 [==============================] - 1s 12ms/step - loss: 2.0972e-04 - accuracy: 1.0000 - val_loss: 1.2514 - val_accuracy: 0.3888\n",
            "Epoch 47/500\n",
            "84/84 [==============================] - 1s 12ms/step - loss: 0.0029 - accuracy: 0.9996 - val_loss: 1.1377 - val_accuracy: 0.4063\n",
            "Epoch 48/500\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 1.8765e-04 - accuracy: 1.0000 - val_loss: 1.2533 - val_accuracy: 0.3827\n",
            "Epoch 49/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 1.3307 - val_accuracy: 0.3634\n",
            "Epoch 50/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0060 - accuracy: 0.9989 - val_loss: 1.3458 - val_accuracy: 0.3748\n",
            "Epoch 51/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 6.1799e-04 - accuracy: 0.9996 - val_loss: 1.8159 - val_accuracy: 0.3336\n",
            "Epoch 52/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0061 - accuracy: 0.9992 - val_loss: 2.0624 - val_accuracy: 0.3468\n",
            "Epoch 53/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 2.0579 - val_accuracy: 0.3529\n",
            "Epoch 54/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 6.9900e-04 - accuracy: 0.9996 - val_loss: 1.6101 - val_accuracy: 0.3520\n",
            "Epoch 55/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 1.3437e-04 - accuracy: 1.0000 - val_loss: 1.5529 - val_accuracy: 0.3529\n",
            "Epoch 56/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 1.4898 - val_accuracy: 0.3704\n",
            "Epoch 57/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 2.8452e-04 - accuracy: 1.0000 - val_loss: 1.5494 - val_accuracy: 0.3590\n",
            "Epoch 58/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0102 - accuracy: 0.9989 - val_loss: 1.7481 - val_accuracy: 0.3870\n",
            "Epoch 59/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 1.9658 - val_accuracy: 0.3678\n",
            "Epoch 60/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0042 - accuracy: 0.9992 - val_loss: 1.3411 - val_accuracy: 0.4335\n",
            "Epoch 61/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 1.3776e-04 - accuracy: 1.0000 - val_loss: 1.3626 - val_accuracy: 0.4264\n",
            "Epoch 62/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 3.4103e-04 - accuracy: 1.0000 - val_loss: 1.3165 - val_accuracy: 0.4282\n",
            "Epoch 63/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0037 - accuracy: 0.9992 - val_loss: 1.3675 - val_accuracy: 0.4054\n",
            "Epoch 64/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 1.8788 - val_accuracy: 0.3538\n",
            "Epoch 65/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 1.8599 - val_accuracy: 0.3468\n",
            "Epoch 66/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 5.0576e-04 - accuracy: 1.0000 - val_loss: 1.7147 - val_accuracy: 0.3634\n",
            "Epoch 67/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 6.1857e-04 - accuracy: 0.9996 - val_loss: 1.6446 - val_accuracy: 0.3529\n",
            "Epoch 68/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 3.9414e-04 - accuracy: 0.9996 - val_loss: 1.7368 - val_accuracy: 0.3564\n",
            "Epoch 69/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0053 - accuracy: 0.9996 - val_loss: 1.5917 - val_accuracy: 0.3625\n",
            "Epoch 70/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0016 - accuracy: 0.9992 - val_loss: 1.5165 - val_accuracy: 0.3468\n",
            "Epoch 71/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0104 - accuracy: 0.9992 - val_loss: 1.5122 - val_accuracy: 0.3573\n",
            "Epoch 72/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 1.9128 - val_accuracy: 0.2968\n",
            "Epoch 73/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0045 - accuracy: 0.9992 - val_loss: 2.1153 - val_accuracy: 0.2942\n",
            "Epoch 74/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0075 - accuracy: 0.9989 - val_loss: 2.2114 - val_accuracy: 0.2942\n",
            "Epoch 75/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0052 - accuracy: 0.9989 - val_loss: 2.0596 - val_accuracy: 0.2951\n",
            "Epoch 76/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 4.2786e-04 - accuracy: 1.0000 - val_loss: 2.2746 - val_accuracy: 0.2986\n",
            "Epoch 77/500\n",
            "84/84 [==============================] - 1s 6ms/step - loss: 0.0039 - accuracy: 0.9992 - val_loss: 3.3710 - val_accuracy: 0.2951\n",
            "Epoch 78/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0023 - accuracy: 0.9989 - val_loss: 4.6473 - val_accuracy: 0.2942\n",
            "Epoch 79/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 2.2512e-04 - accuracy: 1.0000 - val_loss: 4.0233 - val_accuracy: 0.2951\n",
            "Epoch 80/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 3.5769e-04 - accuracy: 1.0000 - val_loss: 4.0093 - val_accuracy: 0.2951\n",
            "Epoch 81/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0065 - accuracy: 0.9996 - val_loss: 3.5165 - val_accuracy: 0.2960\n",
            "Epoch 82/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 3.7867 - val_accuracy: 0.2960\n",
            "Epoch 83/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0048 - accuracy: 0.9992 - val_loss: 3.4174 - val_accuracy: 0.2960\n",
            "Epoch 84/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 2.1716e-04 - accuracy: 1.0000 - val_loss: 3.3770 - val_accuracy: 0.2968\n",
            "Epoch 85/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 1.3185e-04 - accuracy: 1.0000 - val_loss: 3.3475 - val_accuracy: 0.2968\n",
            "Epoch 86/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 3.9117e-04 - accuracy: 1.0000 - val_loss: 3.2343 - val_accuracy: 0.2968\n",
            "Epoch 87/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 2.4693 - val_accuracy: 0.3004\n",
            "Epoch 88/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 2.3254 - val_accuracy: 0.3012\n",
            "Epoch 89/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.4968e-04 - accuracy: 1.0000 - val_loss: 2.4340 - val_accuracy: 0.3004\n",
            "Epoch 90/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 3.0582e-04 - accuracy: 1.0000 - val_loss: 2.4604 - val_accuracy: 0.2986\n",
            "Epoch 91/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 8.6050e-04 - accuracy: 0.9992 - val_loss: 3.5935 - val_accuracy: 0.2951\n",
            "Epoch 92/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 3.3708e-04 - accuracy: 1.0000 - val_loss: 3.2124 - val_accuracy: 0.2951\n",
            "Epoch 93/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 6.4816e-05 - accuracy: 1.0000 - val_loss: 3.0103 - val_accuracy: 0.2942\n",
            "Epoch 94/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0065 - accuracy: 0.9996 - val_loss: 3.0093 - val_accuracy: 0.2951\n",
            "Epoch 95/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 1.3037e-04 - accuracy: 1.0000 - val_loss: 3.1636 - val_accuracy: 0.2977\n",
            "Epoch 96/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.3194e-04 - accuracy: 1.0000 - val_loss: 3.3164 - val_accuracy: 0.2968\n",
            "Epoch 97/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 2.4875e-04 - accuracy: 1.0000 - val_loss: 3.0707 - val_accuracy: 0.2977\n",
            "Epoch 98/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 3.8770e-04 - accuracy: 0.9996 - val_loss: 2.8834 - val_accuracy: 0.2968\n",
            "Epoch 99/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 1.0191e-04 - accuracy: 1.0000 - val_loss: 2.8521 - val_accuracy: 0.2960\n",
            "Epoch 100/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 1.6940e-04 - accuracy: 1.0000 - val_loss: 2.7660 - val_accuracy: 0.2960\n",
            "Epoch 101/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 5.7910e-05 - accuracy: 1.0000 - val_loss: 2.7941 - val_accuracy: 0.2951\n",
            "Epoch 102/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 4.7350e-04 - accuracy: 0.9996 - val_loss: 2.8706 - val_accuracy: 0.2951\n",
            "Epoch 103/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 4.9443e-05 - accuracy: 1.0000 - val_loss: 3.0739 - val_accuracy: 0.2951\n",
            "Epoch 104/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 5.8042e-05 - accuracy: 1.0000 - val_loss: 3.2736 - val_accuracy: 0.2942\n",
            "Epoch 105/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 6.7266e-05 - accuracy: 1.0000 - val_loss: 3.3018 - val_accuracy: 0.2942\n",
            "Epoch 106/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 8.7184e-05 - accuracy: 1.0000 - val_loss: 3.2698 - val_accuracy: 0.2942\n",
            "Epoch 107/500\n",
            "84/84 [==============================] - 1s 6ms/step - loss: 9.2189e-05 - accuracy: 1.0000 - val_loss: 3.2399 - val_accuracy: 0.2951\n",
            "Epoch 108/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.5683e-04 - accuracy: 1.0000 - val_loss: 3.0852 - val_accuracy: 0.2942\n",
            "Epoch 109/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 7.7827e-05 - accuracy: 1.0000 - val_loss: 3.0447 - val_accuracy: 0.2942\n",
            "Epoch 110/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.2275e-04 - accuracy: 1.0000 - val_loss: 3.0701 - val_accuracy: 0.2942\n",
            "Epoch 111/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 4.8033e-05 - accuracy: 1.0000 - val_loss: 3.0641 - val_accuracy: 0.2942\n",
            "Epoch 112/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 9.0091e-05 - accuracy: 1.0000 - val_loss: 3.2372 - val_accuracy: 0.2942\n",
            "Epoch 113/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 9.7561e-05 - accuracy: 1.0000 - val_loss: 3.1995 - val_accuracy: 0.2942\n",
            "Epoch 114/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 3.1119e-05 - accuracy: 1.0000 - val_loss: 3.1854 - val_accuracy: 0.2942\n",
            "Epoch 115/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 3.8015e-05 - accuracy: 1.0000 - val_loss: 3.1799 - val_accuracy: 0.2942\n",
            "Epoch 116/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 9.8268e-05 - accuracy: 1.0000 - val_loss: 3.3926 - val_accuracy: 0.2942\n",
            "Epoch 117/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 4.0562e-05 - accuracy: 1.0000 - val_loss: 3.3749 - val_accuracy: 0.2942\n",
            "Epoch 118/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 2.4443e-05 - accuracy: 1.0000 - val_loss: 3.3832 - val_accuracy: 0.2942\n",
            "Epoch 119/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 2.4394e-04 - accuracy: 1.0000 - val_loss: 4.1480 - val_accuracy: 0.2942\n",
            "Epoch 120/500\n",
            "84/84 [==============================] - 1s 6ms/step - loss: 2.3295e-05 - accuracy: 1.0000 - val_loss: 4.3403 - val_accuracy: 0.2942\n",
            "Epoch 121/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 2.7669e-05 - accuracy: 1.0000 - val_loss: 4.3953 - val_accuracy: 0.2942\n",
            "Epoch 122/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 5.8586e-05 - accuracy: 1.0000 - val_loss: 4.4998 - val_accuracy: 0.2942\n",
            "Epoch 123/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 7.4734e-05 - accuracy: 1.0000 - val_loss: 4.4484 - val_accuracy: 0.2942\n",
            "Epoch 124/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 5.7930e-05 - accuracy: 1.0000 - val_loss: 4.3691 - val_accuracy: 0.2942\n",
            "Epoch 125/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 9.4545e-05 - accuracy: 1.0000 - val_loss: 4.4309 - val_accuracy: 0.2942\n",
            "Epoch 126/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.8623e-05 - accuracy: 1.0000 - val_loss: 4.4970 - val_accuracy: 0.2942\n",
            "Epoch 127/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 1.5130e-05 - accuracy: 1.0000 - val_loss: 4.5025 - val_accuracy: 0.2942\n",
            "Epoch 128/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 7.1507e-04 - accuracy: 0.9996 - val_loss: 3.6214 - val_accuracy: 0.2942\n",
            "Epoch 129/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0014 - accuracy: 0.9992 - val_loss: 5.2020 - val_accuracy: 0.2942\n",
            "Epoch 130/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0134 - accuracy: 0.9992 - val_loss: 5.3005 - val_accuracy: 0.2942\n",
            "Epoch 131/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 9.3938e-04 - accuracy: 0.9992 - val_loss: 4.7985 - val_accuracy: 0.2942\n",
            "Epoch 132/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 2.4294e-04 - accuracy: 1.0000 - val_loss: 4.3787 - val_accuracy: 0.2942\n",
            "Epoch 133/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 4.2752 - val_accuracy: 0.2942\n",
            "Epoch 134/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 4.7809 - val_accuracy: 0.2942\n",
            "Epoch 135/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 4.2484 - val_accuracy: 0.2942\n",
            "Epoch 136/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0058 - accuracy: 0.9985 - val_loss: 4.0440 - val_accuracy: 0.2942\n",
            "Epoch 137/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 4.1767 - val_accuracy: 0.2942\n",
            "Epoch 138/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 4.8022e-04 - accuracy: 1.0000 - val_loss: 4.5429 - val_accuracy: 0.2942\n",
            "Epoch 139/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0022 - accuracy: 0.9992 - val_loss: 4.6783 - val_accuracy: 0.2942\n",
            "Epoch 140/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0039 - accuracy: 0.9992 - val_loss: 4.7854 - val_accuracy: 0.2942\n",
            "Epoch 141/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 1.1957e-04 - accuracy: 1.0000 - val_loss: 4.8368 - val_accuracy: 0.2942\n",
            "Epoch 142/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 1.9542e-04 - accuracy: 1.0000 - val_loss: 4.8990 - val_accuracy: 0.2942\n",
            "Epoch 143/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 4.9330 - val_accuracy: 0.2942\n",
            "Epoch 144/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 9.6173e-04 - accuracy: 0.9996 - val_loss: 4.9410 - val_accuracy: 0.2942\n",
            "Epoch 145/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 5.1089e-04 - accuracy: 0.9996 - val_loss: 5.0354 - val_accuracy: 0.2942\n",
            "Epoch 146/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.0309e-04 - accuracy: 1.0000 - val_loss: 5.0732 - val_accuracy: 0.2942\n",
            "Epoch 147/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 3.3082e-05 - accuracy: 1.0000 - val_loss: 5.0400 - val_accuracy: 0.2942\n",
            "Epoch 148/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 4.5508e-05 - accuracy: 1.0000 - val_loss: 5.1010 - val_accuracy: 0.2942\n",
            "Epoch 149/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 4.4026e-04 - accuracy: 0.9996 - val_loss: 5.1004 - val_accuracy: 0.2942\n",
            "Epoch 150/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 4.9717 - val_accuracy: 0.2942\n",
            "Epoch 151/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 7.8947e-05 - accuracy: 1.0000 - val_loss: 4.9687 - val_accuracy: 0.2942\n",
            "Epoch 152/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 2.9198e-04 - accuracy: 1.0000 - val_loss: 5.0419 - val_accuracy: 0.2942\n",
            "Epoch 153/500\n",
            "84/84 [==============================] - 1s 6ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 5.2337 - val_accuracy: 0.2942\n",
            "Epoch 154/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 1.5961e-04 - accuracy: 1.0000 - val_loss: 5.0846 - val_accuracy: 0.2942\n",
            "Epoch 155/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 6.1885e-05 - accuracy: 1.0000 - val_loss: 5.0526 - val_accuracy: 0.2942\n",
            "Epoch 156/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 9.3803e-05 - accuracy: 1.0000 - val_loss: 5.1181 - val_accuracy: 0.2942\n",
            "Epoch 157/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 6.1499e-04 - accuracy: 0.9996 - val_loss: 4.9596 - val_accuracy: 0.2942\n",
            "Epoch 158/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 2.3532e-04 - accuracy: 1.0000 - val_loss: 5.0497 - val_accuracy: 0.2942\n",
            "Epoch 159/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 3.4339e-04 - accuracy: 1.0000 - val_loss: 5.1121 - val_accuracy: 0.2942\n",
            "Epoch 160/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 5.8905e-05 - accuracy: 1.0000 - val_loss: 5.4281 - val_accuracy: 0.2942\n",
            "Epoch 161/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 5.3531e-05 - accuracy: 1.0000 - val_loss: 5.4643 - val_accuracy: 0.2942\n",
            "Epoch 162/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 4.1629e-05 - accuracy: 1.0000 - val_loss: 5.4169 - val_accuracy: 0.2942\n",
            "Epoch 163/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 7.5673e-04 - accuracy: 0.9996 - val_loss: 5.4673 - val_accuracy: 0.2942\n",
            "Epoch 164/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0050 - accuracy: 0.9996 - val_loss: 5.8304 - val_accuracy: 0.2942\n",
            "Epoch 165/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 1.0044e-04 - accuracy: 1.0000 - val_loss: 5.9038 - val_accuracy: 0.2942\n",
            "Epoch 166/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 4.8430e-05 - accuracy: 1.0000 - val_loss: 5.8967 - val_accuracy: 0.2942\n",
            "Epoch 167/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0047 - accuracy: 0.9996 - val_loss: 5.9357 - val_accuracy: 0.2942\n",
            "Epoch 168/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 3.4585e-04 - accuracy: 1.0000 - val_loss: 5.8722 - val_accuracy: 0.2942\n",
            "Epoch 169/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0031 - accuracy: 0.9992 - val_loss: 5.9029 - val_accuracy: 0.2942\n",
            "Epoch 170/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 6.2209 - val_accuracy: 0.2942\n",
            "Epoch 171/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0104 - accuracy: 0.9985 - val_loss: 8.5958 - val_accuracy: 0.2942\n",
            "Epoch 172/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 7.6265e-05 - accuracy: 1.0000 - val_loss: 7.9969 - val_accuracy: 0.2942\n",
            "Epoch 173/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 1.2693e-04 - accuracy: 1.0000 - val_loss: 7.9438 - val_accuracy: 0.2942\n",
            "Epoch 174/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.3575e-04 - accuracy: 1.0000 - val_loss: 7.8827 - val_accuracy: 0.2942\n",
            "Epoch 175/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 2.2374e-04 - accuracy: 1.0000 - val_loss: 7.9536 - val_accuracy: 0.2942\n",
            "Epoch 176/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 7.8979 - val_accuracy: 0.2942\n",
            "Epoch 177/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.6849e-04 - accuracy: 1.0000 - val_loss: 8.0174 - val_accuracy: 0.2942\n",
            "Epoch 178/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0056 - accuracy: 0.9996 - val_loss: 8.0920 - val_accuracy: 0.2942\n",
            "Epoch 179/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 8.4881e-05 - accuracy: 1.0000 - val_loss: 8.3533 - val_accuracy: 0.2942\n",
            "Epoch 180/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.6949e-04 - accuracy: 1.0000 - val_loss: 8.3465 - val_accuracy: 0.2942\n",
            "Epoch 181/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.2093e-04 - accuracy: 1.0000 - val_loss: 8.4371 - val_accuracy: 0.2942\n",
            "Epoch 182/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 1.1812e-04 - accuracy: 1.0000 - val_loss: 8.4948 - val_accuracy: 0.2942\n",
            "Epoch 183/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 2.1928e-04 - accuracy: 1.0000 - val_loss: 8.3911 - val_accuracy: 0.2942\n",
            "Epoch 184/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 7.2715e-05 - accuracy: 1.0000 - val_loss: 8.4268 - val_accuracy: 0.2942\n",
            "Epoch 185/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 1.0856e-04 - accuracy: 1.0000 - val_loss: 8.3395 - val_accuracy: 0.2942\n",
            "Epoch 186/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 6.6114e-05 - accuracy: 1.0000 - val_loss: 8.3553 - val_accuracy: 0.2942\n",
            "Epoch 187/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 8.0295e-04 - accuracy: 0.9996 - val_loss: 7.7826 - val_accuracy: 0.2942\n",
            "Epoch 188/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 2.1721e-04 - accuracy: 1.0000 - val_loss: 8.0423 - val_accuracy: 0.2942\n",
            "Epoch 189/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 5.5383e-05 - accuracy: 1.0000 - val_loss: 8.0984 - val_accuracy: 0.2942\n",
            "Epoch 190/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 8.0575 - val_accuracy: 0.2942\n",
            "Epoch 191/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 1.6543e-04 - accuracy: 1.0000 - val_loss: 8.1409 - val_accuracy: 0.2942\n",
            "Epoch 192/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 5.2598e-04 - accuracy: 0.9996 - val_loss: 8.0971 - val_accuracy: 0.2942\n",
            "Epoch 193/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 8.1122 - val_accuracy: 0.2942\n",
            "Epoch 194/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0102 - accuracy: 0.9981 - val_loss: 8.2618 - val_accuracy: 0.2942\n",
            "Epoch 195/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0073 - accuracy: 0.9989 - val_loss: 8.0628 - val_accuracy: 0.2942\n",
            "Epoch 196/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 6.7061e-04 - accuracy: 0.9996 - val_loss: 8.1445 - val_accuracy: 0.2942\n",
            "Epoch 197/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.0797e-04 - accuracy: 1.0000 - val_loss: 8.1999 - val_accuracy: 0.2942\n",
            "Epoch 198/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 5.9573e-04 - accuracy: 0.9996 - val_loss: 8.4226 - val_accuracy: 0.2942\n",
            "Epoch 199/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0075 - accuracy: 0.9992 - val_loss: 7.9954 - val_accuracy: 0.2942\n",
            "Epoch 200/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 9.3256e-04 - accuracy: 0.9992 - val_loss: 7.4125 - val_accuracy: 0.2942\n",
            "Epoch 201/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0040 - accuracy: 0.9996 - val_loss: 7.5305 - val_accuracy: 0.2942\n",
            "Epoch 202/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.8986e-04 - accuracy: 1.0000 - val_loss: 7.6948 - val_accuracy: 0.2942\n",
            "Epoch 203/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.1159e-04 - accuracy: 1.0000 - val_loss: 7.7797 - val_accuracy: 0.2942\n",
            "Epoch 204/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0035 - accuracy: 0.9992 - val_loss: 7.3135 - val_accuracy: 0.2942\n",
            "Epoch 205/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0030 - accuracy: 0.9996 - val_loss: 7.0843 - val_accuracy: 0.2942\n",
            "Epoch 206/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 2.9759e-04 - accuracy: 1.0000 - val_loss: 6.9728 - val_accuracy: 0.2942\n",
            "Epoch 207/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 9.8974e-04 - accuracy: 0.9996 - val_loss: 6.9335 - val_accuracy: 0.2942\n",
            "Epoch 208/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 7.1523 - val_accuracy: 0.2942\n",
            "Epoch 209/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 7.0704e-04 - accuracy: 0.9996 - val_loss: 7.1672 - val_accuracy: 0.2942\n",
            "Epoch 210/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 7.0879 - val_accuracy: 0.2942\n",
            "Epoch 211/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0020 - accuracy: 0.9992 - val_loss: 7.5464 - val_accuracy: 0.2942\n",
            "Epoch 212/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 7.3397 - val_accuracy: 0.2942\n",
            "Epoch 213/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 2.3251e-04 - accuracy: 1.0000 - val_loss: 7.4174 - val_accuracy: 0.2942\n",
            "Epoch 214/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0053 - accuracy: 0.9992 - val_loss: 7.3194 - val_accuracy: 0.2942\n",
            "Epoch 215/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 2.1741e-04 - accuracy: 1.0000 - val_loss: 7.3007 - val_accuracy: 0.2942\n",
            "Epoch 216/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 6.4634e-04 - accuracy: 0.9996 - val_loss: 7.2573 - val_accuracy: 0.2942\n",
            "Epoch 217/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 1.3664e-04 - accuracy: 1.0000 - val_loss: 7.2549 - val_accuracy: 0.2942\n",
            "Epoch 218/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.6561e-04 - accuracy: 1.0000 - val_loss: 7.2960 - val_accuracy: 0.2942\n",
            "Epoch 219/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 1.0020e-04 - accuracy: 1.0000 - val_loss: 7.3170 - val_accuracy: 0.2942\n",
            "Epoch 220/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 6.8787e-05 - accuracy: 1.0000 - val_loss: 7.3561 - val_accuracy: 0.2942\n",
            "Epoch 221/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0151 - accuracy: 0.9977 - val_loss: 7.6732 - val_accuracy: 0.2942\n",
            "Epoch 222/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 7.0993 - val_accuracy: 0.2942\n",
            "Epoch 223/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0016 - accuracy: 0.9992 - val_loss: 6.7298 - val_accuracy: 0.2942\n",
            "Epoch 224/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 2.1423e-04 - accuracy: 1.0000 - val_loss: 6.8046 - val_accuracy: 0.2942\n",
            "Epoch 225/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0023 - accuracy: 0.9989 - val_loss: 6.9346 - val_accuracy: 0.2942\n",
            "Epoch 226/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 1.2630e-04 - accuracy: 1.0000 - val_loss: 6.9309 - val_accuracy: 0.2942\n",
            "Epoch 227/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 6.9378 - val_accuracy: 0.2942\n",
            "Epoch 228/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 6.5091e-05 - accuracy: 1.0000 - val_loss: 7.0571 - val_accuracy: 0.2942\n",
            "Epoch 229/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 8.6969e-05 - accuracy: 1.0000 - val_loss: 7.1707 - val_accuracy: 0.2942\n",
            "Epoch 230/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 3.5627e-04 - accuracy: 0.9996 - val_loss: 7.1364 - val_accuracy: 0.2942\n",
            "Epoch 231/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 7.1030 - val_accuracy: 0.2942\n",
            "Epoch 232/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 7.1644 - val_accuracy: 0.2942\n",
            "Epoch 233/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 9.5015e-04 - accuracy: 0.9996 - val_loss: 7.2084 - val_accuracy: 0.2942\n",
            "Epoch 234/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 7.4168e-05 - accuracy: 1.0000 - val_loss: 7.2099 - val_accuracy: 0.2942\n",
            "Epoch 235/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.2428e-04 - accuracy: 1.0000 - val_loss: 7.2451 - val_accuracy: 0.2942\n",
            "Epoch 236/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 9.6826e-05 - accuracy: 1.0000 - val_loss: 7.3251 - val_accuracy: 0.2942\n",
            "Epoch 237/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.5316e-04 - accuracy: 1.0000 - val_loss: 7.3470 - val_accuracy: 0.2942\n",
            "Epoch 238/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 9.1212e-05 - accuracy: 1.0000 - val_loss: 7.4217 - val_accuracy: 0.2942\n",
            "Epoch 239/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 3.5607e-05 - accuracy: 1.0000 - val_loss: 7.5114 - val_accuracy: 0.2942\n",
            "Epoch 240/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 3.7239e-05 - accuracy: 1.0000 - val_loss: 7.5063 - val_accuracy: 0.2942\n",
            "Epoch 241/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 5.7936e-05 - accuracy: 1.0000 - val_loss: 7.5305 - val_accuracy: 0.2942\n",
            "Epoch 242/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 2.1425e-04 - accuracy: 1.0000 - val_loss: 7.5573 - val_accuracy: 0.2942\n",
            "Epoch 243/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 3.7073e-05 - accuracy: 1.0000 - val_loss: 7.6153 - val_accuracy: 0.2942\n",
            "Epoch 244/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 6.5362e-04 - accuracy: 0.9996 - val_loss: 7.4924 - val_accuracy: 0.2942\n",
            "Epoch 245/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 3.0172e-04 - accuracy: 1.0000 - val_loss: 7.8259 - val_accuracy: 0.2942\n",
            "Epoch 246/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 3.2960e-05 - accuracy: 1.0000 - val_loss: 7.9751 - val_accuracy: 0.2942\n",
            "Epoch 247/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0046 - accuracy: 0.9992 - val_loss: 8.1509 - val_accuracy: 0.2942\n",
            "Epoch 248/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0015 - accuracy: 0.9992 - val_loss: 7.6365 - val_accuracy: 0.2942\n",
            "Epoch 249/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 2.4330e-04 - accuracy: 1.0000 - val_loss: 7.7866 - val_accuracy: 0.2942\n",
            "Epoch 250/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 8.9707e-04 - accuracy: 0.9996 - val_loss: 7.7165 - val_accuracy: 0.2942\n",
            "Epoch 251/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0029 - accuracy: 0.9992 - val_loss: 7.8373 - val_accuracy: 0.2942\n",
            "Epoch 252/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 3.4471e-04 - accuracy: 1.0000 - val_loss: 7.8716 - val_accuracy: 0.2942\n",
            "Epoch 253/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.9513e-04 - accuracy: 1.0000 - val_loss: 7.7348 - val_accuracy: 0.2942\n",
            "Epoch 254/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 8.0704e-05 - accuracy: 1.0000 - val_loss: 7.7630 - val_accuracy: 0.2942\n",
            "Epoch 255/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 1.1788e-04 - accuracy: 1.0000 - val_loss: 7.7989 - val_accuracy: 0.2942\n",
            "Epoch 256/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 4.9343e-04 - accuracy: 0.9996 - val_loss: 7.9133 - val_accuracy: 0.2942\n",
            "Epoch 257/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 4.0231e-04 - accuracy: 0.9996 - val_loss: 7.8596 - val_accuracy: 0.2942\n",
            "Epoch 258/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0020 - accuracy: 0.9992 - val_loss: 8.0029 - val_accuracy: 0.2942\n",
            "Epoch 259/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 2.1432e-04 - accuracy: 1.0000 - val_loss: 8.0338 - val_accuracy: 0.2942\n",
            "Epoch 260/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 1.5411e-04 - accuracy: 1.0000 - val_loss: 8.2211 - val_accuracy: 0.2942\n",
            "Epoch 261/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 9.5487e-05 - accuracy: 1.0000 - val_loss: 8.1937 - val_accuracy: 0.2942\n",
            "Epoch 262/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 4.9955e-05 - accuracy: 1.0000 - val_loss: 8.1826 - val_accuracy: 0.2942\n",
            "Epoch 263/500\n",
            "84/84 [==============================] - 1s 12ms/step - loss: 3.9796e-05 - accuracy: 1.0000 - val_loss: 8.2640 - val_accuracy: 0.2942\n",
            "Epoch 264/500\n",
            "84/84 [==============================] - 1s 12ms/step - loss: 2.4844e-05 - accuracy: 1.0000 - val_loss: 8.2308 - val_accuracy: 0.2942\n",
            "Epoch 265/500\n",
            "84/84 [==============================] - 1s 12ms/step - loss: 2.9351e-04 - accuracy: 1.0000 - val_loss: 9.2943 - val_accuracy: 0.2942\n",
            "Epoch 266/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 9.7368e-04 - accuracy: 0.9996 - val_loss: 8.9918 - val_accuracy: 0.2942\n",
            "Epoch 267/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 8.4763e-05 - accuracy: 1.0000 - val_loss: 7.6548 - val_accuracy: 0.2942\n",
            "Epoch 268/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 4.1763e-05 - accuracy: 1.0000 - val_loss: 7.7633 - val_accuracy: 0.2942\n",
            "Epoch 269/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 5.3825e-04 - accuracy: 0.9996 - val_loss: 7.7834 - val_accuracy: 0.2942\n",
            "Epoch 270/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 5.5783e-04 - accuracy: 0.9996 - val_loss: 7.6697 - val_accuracy: 0.2942\n",
            "Epoch 271/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0112 - accuracy: 0.9989 - val_loss: 7.8452 - val_accuracy: 0.2942\n",
            "Epoch 272/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0041 - accuracy: 0.9996 - val_loss: 7.8396 - val_accuracy: 0.2942\n",
            "Epoch 273/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 2.5549e-04 - accuracy: 1.0000 - val_loss: 7.9178 - val_accuracy: 0.2942\n",
            "Epoch 274/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 2.9310e-04 - accuracy: 1.0000 - val_loss: 7.9578 - val_accuracy: 0.2942\n",
            "Epoch 275/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0045 - accuracy: 0.9992 - val_loss: 7.9567 - val_accuracy: 0.2942\n",
            "Epoch 276/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0059 - accuracy: 0.9996 - val_loss: 7.8963 - val_accuracy: 0.2942\n",
            "Epoch 277/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0011 - accuracy: 0.9992 - val_loss: 8.0691 - val_accuracy: 0.2942\n",
            "Epoch 278/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 9.0613 - val_accuracy: 0.2942\n",
            "Epoch 279/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 4.2198e-04 - accuracy: 1.0000 - val_loss: 8.9999 - val_accuracy: 0.2942\n",
            "Epoch 280/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 8.0349e-05 - accuracy: 1.0000 - val_loss: 9.0643 - val_accuracy: 0.2942\n",
            "Epoch 281/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.0484e-04 - accuracy: 1.0000 - val_loss: 9.0634 - val_accuracy: 0.2942\n",
            "Epoch 282/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 9.0164e-05 - accuracy: 1.0000 - val_loss: 8.9852 - val_accuracy: 0.2942\n",
            "Epoch 283/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 9.0826 - val_accuracy: 0.2942\n",
            "Epoch 284/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.7828e-04 - accuracy: 1.0000 - val_loss: 9.1494 - val_accuracy: 0.2942\n",
            "Epoch 285/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 8.8957e-04 - accuracy: 0.9996 - val_loss: 9.0448 - val_accuracy: 0.2942\n",
            "Epoch 286/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 8.1722e-05 - accuracy: 1.0000 - val_loss: 8.9453 - val_accuracy: 0.2942\n",
            "Epoch 287/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 9.2068 - val_accuracy: 0.2942\n",
            "Epoch 288/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 1.2961e-04 - accuracy: 1.0000 - val_loss: 9.0908 - val_accuracy: 0.2942\n",
            "Epoch 289/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0013 - accuracy: 0.9992 - val_loss: 9.0118 - val_accuracy: 0.2942\n",
            "Epoch 290/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0029 - accuracy: 0.9992 - val_loss: 9.3032 - val_accuracy: 0.2942\n",
            "Epoch 291/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 1.4796e-04 - accuracy: 1.0000 - val_loss: 9.3176 - val_accuracy: 0.2942\n",
            "Epoch 292/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 1.4067e-04 - accuracy: 1.0000 - val_loss: 9.1373 - val_accuracy: 0.2942\n",
            "Epoch 293/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.0387e-04 - accuracy: 1.0000 - val_loss: 9.1503 - val_accuracy: 0.2942\n",
            "Epoch 294/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 5.7967e-05 - accuracy: 1.0000 - val_loss: 9.2778 - val_accuracy: 0.2942\n",
            "Epoch 295/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0022 - accuracy: 0.9992 - val_loss: 9.1336 - val_accuracy: 0.2942\n",
            "Epoch 296/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0058 - accuracy: 0.9996 - val_loss: 8.9615 - val_accuracy: 0.2942\n",
            "Epoch 297/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.5022e-04 - accuracy: 1.0000 - val_loss: 8.7855 - val_accuracy: 0.2942\n",
            "Epoch 298/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 1.3475e-04 - accuracy: 1.0000 - val_loss: 8.8278 - val_accuracy: 0.2942\n",
            "Epoch 299/500\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 3.1935e-04 - accuracy: 1.0000 - val_loss: 8.9428 - val_accuracy: 0.2942\n",
            "Epoch 300/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.4413e-04 - accuracy: 1.0000 - val_loss: 8.9445 - val_accuracy: 0.2942\n",
            "Epoch 301/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 7.0267e-04 - accuracy: 0.9996 - val_loss: 9.4929 - val_accuracy: 0.2942\n",
            "Epoch 302/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 3.9728e-05 - accuracy: 1.0000 - val_loss: 9.5486 - val_accuracy: 0.2942\n",
            "Epoch 303/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 9.1505 - val_accuracy: 0.2942\n",
            "Epoch 304/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 4.1155e-05 - accuracy: 1.0000 - val_loss: 9.1508 - val_accuracy: 0.2942\n",
            "Epoch 305/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 3.7510e-05 - accuracy: 1.0000 - val_loss: 9.2252 - val_accuracy: 0.2942\n",
            "Epoch 306/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 2.0744e-04 - accuracy: 1.0000 - val_loss: 9.4275 - val_accuracy: 0.2942\n",
            "Epoch 307/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 6.3644e-05 - accuracy: 1.0000 - val_loss: 9.4227 - val_accuracy: 0.2942\n",
            "Epoch 308/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 4.1255e-05 - accuracy: 1.0000 - val_loss: 9.3850 - val_accuracy: 0.2942\n",
            "Epoch 309/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 3.3490e-04 - accuracy: 0.9996 - val_loss: 9.2966 - val_accuracy: 0.2942\n",
            "Epoch 310/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0028 - accuracy: 0.9996 - val_loss: 8.5644 - val_accuracy: 0.2942\n",
            "Epoch 311/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 7.0050e-05 - accuracy: 1.0000 - val_loss: 8.6590 - val_accuracy: 0.2942\n",
            "Epoch 312/500\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 1.7787e-04 - accuracy: 1.0000 - val_loss: 8.7106 - val_accuracy: 0.2942\n",
            "Epoch 313/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.9789e-04 - accuracy: 1.0000 - val_loss: 8.6490 - val_accuracy: 0.2942\n",
            "Epoch 314/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0094 - accuracy: 0.9985 - val_loss: 9.2913 - val_accuracy: 0.2942\n",
            "Epoch 315/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0024 - accuracy: 0.9992 - val_loss: 9.3116 - val_accuracy: 0.2942\n",
            "Epoch 316/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 7.0553e-05 - accuracy: 1.0000 - val_loss: 9.1130 - val_accuracy: 0.2942\n",
            "Epoch 317/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 8.7681 - val_accuracy: 0.2942\n",
            "Epoch 318/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0021 - accuracy: 0.9989 - val_loss: 8.9358 - val_accuracy: 0.2942\n",
            "Epoch 319/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 2.8977e-04 - accuracy: 1.0000 - val_loss: 8.9802 - val_accuracy: 0.2942\n",
            "Epoch 320/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 1.6901e-04 - accuracy: 1.0000 - val_loss: 9.0210 - val_accuracy: 0.2942\n",
            "Epoch 321/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0037 - accuracy: 0.9992 - val_loss: 8.9629 - val_accuracy: 0.2942\n",
            "Epoch 322/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 8.8168 - val_accuracy: 0.2942\n",
            "Epoch 323/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0023 - accuracy: 0.9992 - val_loss: 8.6624 - val_accuracy: 0.2942\n",
            "Epoch 324/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 2.8716e-04 - accuracy: 1.0000 - val_loss: 8.6291 - val_accuracy: 0.2942\n",
            "Epoch 325/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 3.2328e-04 - accuracy: 1.0000 - val_loss: 8.5206 - val_accuracy: 0.2942\n",
            "Epoch 326/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 6.9453e-04 - accuracy: 0.9996 - val_loss: 8.7092 - val_accuracy: 0.2942\n",
            "Epoch 327/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0029 - accuracy: 0.9996 - val_loss: 8.7013 - val_accuracy: 0.2942\n",
            "Epoch 328/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 9.4069 - val_accuracy: 0.2942\n",
            "Epoch 329/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 8.5660 - val_accuracy: 0.2942\n",
            "Epoch 330/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0035 - accuracy: 0.9992 - val_loss: 8.3492 - val_accuracy: 0.2942\n",
            "Epoch 331/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 3.6312e-04 - accuracy: 1.0000 - val_loss: 8.6113 - val_accuracy: 0.2942\n",
            "Epoch 332/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0051 - accuracy: 0.9996 - val_loss: 8.8252 - val_accuracy: 0.2942\n",
            "Epoch 333/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0043 - accuracy: 0.9985 - val_loss: 8.9773 - val_accuracy: 0.2942\n",
            "Epoch 334/500\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0015 - accuracy: 0.9989 - val_loss: 9.2898 - val_accuracy: 0.2942\n",
            "Epoch 335/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 9.2946 - val_accuracy: 0.2942\n",
            "Epoch 336/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 2.3176e-04 - accuracy: 1.0000 - val_loss: 9.2046 - val_accuracy: 0.2942\n",
            "Epoch 337/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.8872e-04 - accuracy: 1.0000 - val_loss: 9.3458 - val_accuracy: 0.2942\n",
            "Epoch 338/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 4.4546e-05 - accuracy: 1.0000 - val_loss: 9.2084 - val_accuracy: 0.2942\n",
            "Epoch 339/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0057 - accuracy: 0.9996 - val_loss: 9.1888 - val_accuracy: 0.2942\n",
            "Epoch 340/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 6.3562e-05 - accuracy: 1.0000 - val_loss: 9.1806 - val_accuracy: 0.2942\n",
            "Epoch 341/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 7.3327e-05 - accuracy: 1.0000 - val_loss: 9.1690 - val_accuracy: 0.2942\n",
            "Epoch 342/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0062 - accuracy: 0.9996 - val_loss: 9.2122 - val_accuracy: 0.2942\n",
            "Epoch 343/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 6.5967e-04 - accuracy: 1.0000 - val_loss: 9.2536 - val_accuracy: 0.2942\n",
            "Epoch 344/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.5927e-04 - accuracy: 1.0000 - val_loss: 9.2970 - val_accuracy: 0.2942\n",
            "Epoch 345/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 1.5164e-04 - accuracy: 1.0000 - val_loss: 9.2706 - val_accuracy: 0.2942\n",
            "Epoch 346/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.9696e-04 - accuracy: 1.0000 - val_loss: 9.2898 - val_accuracy: 0.2942\n",
            "Epoch 347/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 8.8989e-05 - accuracy: 1.0000 - val_loss: 9.2713 - val_accuracy: 0.2942\n",
            "Epoch 348/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0034 - accuracy: 0.9996 - val_loss: 9.3292 - val_accuracy: 0.2942\n",
            "Epoch 349/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0035 - accuracy: 0.9996 - val_loss: 9.3365 - val_accuracy: 0.2942\n",
            "Epoch 350/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 7.2484e-05 - accuracy: 1.0000 - val_loss: 9.2575 - val_accuracy: 0.2942\n",
            "Epoch 351/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.8683e-04 - accuracy: 1.0000 - val_loss: 9.2231 - val_accuracy: 0.2942\n",
            "Epoch 352/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.6687e-04 - accuracy: 1.0000 - val_loss: 9.2001 - val_accuracy: 0.2942\n",
            "Epoch 353/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 6.1536e-05 - accuracy: 1.0000 - val_loss: 9.2382 - val_accuracy: 0.2942\n",
            "Epoch 354/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0029 - accuracy: 0.9996 - val_loss: 10.0173 - val_accuracy: 0.2942\n",
            "Epoch 355/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0028 - accuracy: 0.9989 - val_loss: 9.2378 - val_accuracy: 0.2942\n",
            "Epoch 356/500\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0026 - accuracy: 0.9992 - val_loss: 11.1376 - val_accuracy: 0.2942\n",
            "Epoch 357/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 1.2475e-04 - accuracy: 1.0000 - val_loss: 11.4144 - val_accuracy: 0.2942\n",
            "Epoch 358/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.7155e-04 - accuracy: 1.0000 - val_loss: 11.4128 - val_accuracy: 0.2942\n",
            "Epoch 359/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 4.3242e-04 - accuracy: 0.9996 - val_loss: 11.4392 - val_accuracy: 0.2942\n",
            "Epoch 360/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0025 - accuracy: 0.9989 - val_loss: 11.1762 - val_accuracy: 0.2942\n",
            "Epoch 361/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 4.5620e-04 - accuracy: 1.0000 - val_loss: 10.9634 - val_accuracy: 0.2942\n",
            "Epoch 362/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 11.2925 - val_accuracy: 0.2942\n",
            "Epoch 363/500\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 1.9601e-04 - accuracy: 1.0000 - val_loss: 11.2716 - val_accuracy: 0.2942\n",
            "Epoch 364/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 6.6727e-05 - accuracy: 1.0000 - val_loss: 11.2291 - val_accuracy: 0.2942\n",
            "Epoch 365/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 6.2094e-04 - accuracy: 0.9996 - val_loss: 11.3832 - val_accuracy: 0.2942\n",
            "Epoch 366/500\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 8.8562e-05 - accuracy: 1.0000 - val_loss: 11.7366 - val_accuracy: 0.2942\n",
            "Epoch 367/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 1.5298e-04 - accuracy: 1.0000 - val_loss: 11.6930 - val_accuracy: 0.2942\n",
            "Epoch 368/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 5.8790e-05 - accuracy: 1.0000 - val_loss: 11.6371 - val_accuracy: 0.2942\n",
            "Epoch 369/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 7.1193e-05 - accuracy: 1.0000 - val_loss: 11.5553 - val_accuracy: 0.2942\n",
            "Epoch 370/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 3.2354e-05 - accuracy: 1.0000 - val_loss: 11.5636 - val_accuracy: 0.2942\n",
            "Epoch 371/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 2.9240e-05 - accuracy: 1.0000 - val_loss: 11.6054 - val_accuracy: 0.2942\n",
            "Epoch 372/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 5.2827e-05 - accuracy: 1.0000 - val_loss: 11.5807 - val_accuracy: 0.2942\n",
            "Epoch 373/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0048 - accuracy: 0.9992 - val_loss: 11.4040 - val_accuracy: 0.2942\n",
            "Epoch 374/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0069 - accuracy: 0.9992 - val_loss: 11.3313 - val_accuracy: 0.2942\n",
            "Epoch 375/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0033 - accuracy: 0.9992 - val_loss: 11.4846 - val_accuracy: 0.2942\n",
            "Epoch 376/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 6.5852e-04 - accuracy: 0.9996 - val_loss: 11.5315 - val_accuracy: 0.2942\n",
            "Epoch 377/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 6.8833e-05 - accuracy: 1.0000 - val_loss: 11.4540 - val_accuracy: 0.2942\n",
            "Epoch 378/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 2.2625e-05 - accuracy: 1.0000 - val_loss: 11.3926 - val_accuracy: 0.2942\n",
            "Epoch 379/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0162 - accuracy: 0.9977 - val_loss: 11.1572 - val_accuracy: 0.2942\n",
            "Epoch 380/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 4.4668e-04 - accuracy: 1.0000 - val_loss: 10.6231 - val_accuracy: 0.2942\n",
            "Epoch 381/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 5.4714e-04 - accuracy: 0.9996 - val_loss: 10.7754 - val_accuracy: 0.2942\n",
            "Epoch 382/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0038 - accuracy: 0.9996 - val_loss: 11.1238 - val_accuracy: 0.2942\n",
            "Epoch 383/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 5.5451e-05 - accuracy: 1.0000 - val_loss: 11.1373 - val_accuracy: 0.2942\n",
            "Epoch 384/500\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 1.3393e-04 - accuracy: 1.0000 - val_loss: 11.1852 - val_accuracy: 0.2942\n",
            "Epoch 385/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 2.1177e-04 - accuracy: 1.0000 - val_loss: 11.1637 - val_accuracy: 0.2942\n",
            "Epoch 386/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 4.2694e-04 - accuracy: 1.0000 - val_loss: 11.3589 - val_accuracy: 0.2942\n",
            "Epoch 387/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.5121e-04 - accuracy: 1.0000 - val_loss: 11.4209 - val_accuracy: 0.2942\n",
            "Epoch 388/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.2724e-04 - accuracy: 1.0000 - val_loss: 11.3329 - val_accuracy: 0.2942\n",
            "Epoch 389/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0270 - accuracy: 0.9989 - val_loss: 11.0126 - val_accuracy: 0.2942\n",
            "Epoch 390/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 7.4657e-04 - accuracy: 1.0000 - val_loss: 9.3456 - val_accuracy: 0.2942\n",
            "Epoch 391/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 10.2505 - val_accuracy: 0.2942\n",
            "Epoch 392/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 5.5196e-04 - accuracy: 0.9996 - val_loss: 10.2733 - val_accuracy: 0.2942\n",
            "Epoch 393/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0086 - accuracy: 0.9989 - val_loss: 10.1020 - val_accuracy: 0.2942\n",
            "Epoch 394/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 10.2175 - val_accuracy: 0.2942\n",
            "Epoch 395/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 2.6556e-04 - accuracy: 1.0000 - val_loss: 10.1871 - val_accuracy: 0.2942\n",
            "Epoch 396/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 3.0921e-04 - accuracy: 1.0000 - val_loss: 10.1505 - val_accuracy: 0.2942\n",
            "Epoch 397/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 2.2603e-04 - accuracy: 1.0000 - val_loss: 10.1594 - val_accuracy: 0.2942\n",
            "Epoch 398/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 1.9651e-04 - accuracy: 1.0000 - val_loss: 10.1566 - val_accuracy: 0.2942\n",
            "Epoch 399/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 6.8845e-04 - accuracy: 0.9996 - val_loss: 10.2308 - val_accuracy: 0.2942\n",
            "Epoch 400/500\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 10.0935 - val_accuracy: 0.2942\n",
            "Epoch 401/500\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 6.6677e-04 - accuracy: 1.0000 - val_loss: 10.1421 - val_accuracy: 0.2942\n",
            "Epoch 402/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 1.9834e-04 - accuracy: 1.0000 - val_loss: 10.3223 - val_accuracy: 0.2942\n",
            "Epoch 403/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0098 - accuracy: 0.9992 - val_loss: 10.4583 - val_accuracy: 0.2942\n",
            "Epoch 404/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0036 - accuracy: 0.9989 - val_loss: 10.9860 - val_accuracy: 0.2942\n",
            "Epoch 405/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 7.5844e-04 - accuracy: 1.0000 - val_loss: 11.9116 - val_accuracy: 0.2942\n",
            "Epoch 406/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 11.8212 - val_accuracy: 0.2942\n",
            "Epoch 407/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 9.1889e-04 - accuracy: 1.0000 - val_loss: 11.8284 - val_accuracy: 0.2942\n",
            "Epoch 408/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 3.9308e-04 - accuracy: 1.0000 - val_loss: 11.9680 - val_accuracy: 0.2942\n",
            "Epoch 409/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0035 - accuracy: 0.9996 - val_loss: 10.8628 - val_accuracy: 0.2942\n",
            "Epoch 410/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 1.3842e-04 - accuracy: 1.0000 - val_loss: 10.7194 - val_accuracy: 0.2942\n",
            "Epoch 411/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 2.4915e-04 - accuracy: 1.0000 - val_loss: 10.7306 - val_accuracy: 0.2942\n",
            "Epoch 412/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 10.7476 - val_accuracy: 0.2942\n",
            "Epoch 413/500\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 2.5217e-04 - accuracy: 1.0000 - val_loss: 10.7756 - val_accuracy: 0.2942\n",
            "Epoch 414/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 6.3453e-04 - accuracy: 0.9996 - val_loss: 11.3886 - val_accuracy: 0.2942\n",
            "Epoch 415/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0012 - accuracy: 0.9992 - val_loss: 11.3859 - val_accuracy: 0.2942\n",
            "Epoch 416/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0037 - accuracy: 0.9989 - val_loss: 10.9877 - val_accuracy: 0.2942\n",
            "Epoch 417/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 2.8062e-04 - accuracy: 1.0000 - val_loss: 10.8300 - val_accuracy: 0.2942\n",
            "Epoch 418/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 2.1736e-04 - accuracy: 1.0000 - val_loss: 10.8354 - val_accuracy: 0.2942\n",
            "Epoch 419/500\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 1.5468e-04 - accuracy: 1.0000 - val_loss: 10.7920 - val_accuracy: 0.2942\n",
            "Epoch 420/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 2.5228e-04 - accuracy: 1.0000 - val_loss: 10.7875 - val_accuracy: 0.2942\n",
            "Epoch 421/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 2.2354e-04 - accuracy: 1.0000 - val_loss: 11.0664 - val_accuracy: 0.2942\n",
            "Epoch 422/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.6602e-04 - accuracy: 1.0000 - val_loss: 11.0709 - val_accuracy: 0.2942\n",
            "Epoch 423/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 3.2609e-04 - accuracy: 1.0000 - val_loss: 11.0411 - val_accuracy: 0.2942\n",
            "Epoch 424/500\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0016 - accuracy: 0.9992 - val_loss: 11.8056 - val_accuracy: 0.2942\n",
            "Epoch 425/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.1314e-04 - accuracy: 1.0000 - val_loss: 11.8646 - val_accuracy: 0.2942\n",
            "Epoch 426/500\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 3.8834e-04 - accuracy: 1.0000 - val_loss: 11.8216 - val_accuracy: 0.2942\n",
            "Epoch 427/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 1.3044e-04 - accuracy: 1.0000 - val_loss: 11.6976 - val_accuracy: 0.2942\n",
            "Epoch 428/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 6.9502e-05 - accuracy: 1.0000 - val_loss: 11.7398 - val_accuracy: 0.2942\n",
            "Epoch 429/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 6.8371e-04 - accuracy: 0.9996 - val_loss: 11.6924 - val_accuracy: 0.2942\n",
            "Epoch 430/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 7.4407e-04 - accuracy: 0.9996 - val_loss: 11.3865 - val_accuracy: 0.2942\n",
            "Epoch 431/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0051 - accuracy: 0.9996 - val_loss: 11.1114 - val_accuracy: 0.2942\n",
            "Epoch 432/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 5.1945e-04 - accuracy: 1.0000 - val_loss: 10.9010 - val_accuracy: 0.2942\n",
            "Epoch 433/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 8.3362e-04 - accuracy: 0.9996 - val_loss: 10.1468 - val_accuracy: 0.2942\n",
            "Epoch 434/500\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 8.3021e-04 - accuracy: 0.9996 - val_loss: 10.1903 - val_accuracy: 0.2942\n",
            "Epoch 435/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 10.2882 - val_accuracy: 0.2942\n",
            "Epoch 436/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 9.1601e-04 - accuracy: 0.9996 - val_loss: 10.9859 - val_accuracy: 0.2942\n",
            "Epoch 437/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 2.3003e-04 - accuracy: 1.0000 - val_loss: 10.9124 - val_accuracy: 0.2942\n",
            "Epoch 438/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 10.8641 - val_accuracy: 0.2942\n",
            "Epoch 439/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 0.0036 - accuracy: 0.9996 - val_loss: 10.8989 - val_accuracy: 0.2942\n",
            "Epoch 440/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 2.1399e-04 - accuracy: 1.0000 - val_loss: 11.2714 - val_accuracy: 0.2942\n",
            "Epoch 441/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.2170e-04 - accuracy: 1.0000 - val_loss: 11.1432 - val_accuracy: 0.2942\n",
            "Epoch 442/500\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 1.1240e-04 - accuracy: 1.0000 - val_loss: 11.1089 - val_accuracy: 0.2942\n",
            "Epoch 443/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.8635e-04 - accuracy: 1.0000 - val_loss: 11.1583 - val_accuracy: 0.2942\n",
            "Epoch 444/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 9.1732e-05 - accuracy: 1.0000 - val_loss: 11.2538 - val_accuracy: 0.2942\n",
            "Epoch 445/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 7.1364e-05 - accuracy: 1.0000 - val_loss: 11.2470 - val_accuracy: 0.2942\n",
            "Epoch 446/500\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 1.0132e-04 - accuracy: 1.0000 - val_loss: 11.1820 - val_accuracy: 0.2942\n",
            "Epoch 447/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 2.0314e-04 - accuracy: 1.0000 - val_loss: 11.0549 - val_accuracy: 0.2942\n",
            "Epoch 448/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 3.4310e-04 - accuracy: 0.9996 - val_loss: 10.7719 - val_accuracy: 0.2942\n",
            "Epoch 449/500\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 1.4834e-04 - accuracy: 1.0000 - val_loss: 10.6915 - val_accuracy: 0.2942\n",
            "Epoch 450/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 4.8689e-05 - accuracy: 1.0000 - val_loss: 10.6762 - val_accuracy: 0.2942\n",
            "Epoch 451/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 10.6802 - val_accuracy: 0.2942\n",
            "Epoch 452/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.8214e-04 - accuracy: 1.0000 - val_loss: 10.5088 - val_accuracy: 0.2942\n",
            "Epoch 453/500\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0047 - accuracy: 0.9992 - val_loss: 10.4129 - val_accuracy: 0.2942\n",
            "Epoch 454/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 3.1117e-04 - accuracy: 1.0000 - val_loss: 10.3567 - val_accuracy: 0.2942\n",
            "Epoch 455/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 6.7613e-05 - accuracy: 1.0000 - val_loss: 10.2916 - val_accuracy: 0.2942\n",
            "Epoch 456/500\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 9.0514e-05 - accuracy: 1.0000 - val_loss: 10.3135 - val_accuracy: 0.2942\n",
            "Epoch 457/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 5.9524e-05 - accuracy: 1.0000 - val_loss: 10.2994 - val_accuracy: 0.2942\n",
            "Epoch 458/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 1.4088e-04 - accuracy: 1.0000 - val_loss: 10.3421 - val_accuracy: 0.2942\n",
            "Epoch 459/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 4.7905e-05 - accuracy: 1.0000 - val_loss: 10.2627 - val_accuracy: 0.2942\n",
            "Epoch 460/500\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 4.5632e-04 - accuracy: 0.9996 - val_loss: 10.4283 - val_accuracy: 0.2942\n",
            "Epoch 461/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 11.0357 - val_accuracy: 0.2942\n",
            "Epoch 462/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 1.5476e-04 - accuracy: 1.0000 - val_loss: 11.1455 - val_accuracy: 0.2942\n",
            "Epoch 463/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 8.8438e-05 - accuracy: 1.0000 - val_loss: 11.1588 - val_accuracy: 0.2942\n",
            "Epoch 464/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 7.3366e-05 - accuracy: 1.0000 - val_loss: 11.2860 - val_accuracy: 0.2942\n",
            "Epoch 465/500\n",
            "84/84 [==============================] - 1s 10ms/step - loss: 1.8415e-04 - accuracy: 1.0000 - val_loss: 11.1921 - val_accuracy: 0.2942\n",
            "Epoch 466/500\n",
            "84/84 [==============================] - 1s 12ms/step - loss: 3.0643e-05 - accuracy: 1.0000 - val_loss: 11.2041 - val_accuracy: 0.2942\n",
            "Epoch 467/500\n",
            "84/84 [==============================] - 1s 13ms/step - loss: 0.0038 - accuracy: 0.9992 - val_loss: 11.6580 - val_accuracy: 0.2942\n",
            "Epoch 468/500\n",
            "84/84 [==============================] - 1s 10ms/step - loss: 6.6245e-05 - accuracy: 1.0000 - val_loss: 11.6009 - val_accuracy: 0.2942\n",
            "Epoch 469/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 7.6348e-05 - accuracy: 1.0000 - val_loss: 11.6062 - val_accuracy: 0.2942\n",
            "Epoch 470/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 5.1652e-04 - accuracy: 0.9996 - val_loss: 11.5621 - val_accuracy: 0.2942\n",
            "Epoch 471/500\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 2.7642e-04 - accuracy: 1.0000 - val_loss: 12.3411 - val_accuracy: 0.2942\n",
            "Epoch 472/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0039 - accuracy: 0.9996 - val_loss: 11.9126 - val_accuracy: 0.2942\n",
            "Epoch 473/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 7.8085e-05 - accuracy: 1.0000 - val_loss: 11.8529 - val_accuracy: 0.2942\n",
            "Epoch 474/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0018 - accuracy: 0.9992 - val_loss: 11.7966 - val_accuracy: 0.2942\n",
            "Epoch 475/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.7544e-04 - accuracy: 1.0000 - val_loss: 11.7830 - val_accuracy: 0.2942\n",
            "Epoch 476/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 5.8483e-05 - accuracy: 1.0000 - val_loss: 11.8395 - val_accuracy: 0.2942\n",
            "Epoch 477/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 5.2554e-05 - accuracy: 1.0000 - val_loss: 11.8456 - val_accuracy: 0.2942\n",
            "Epoch 478/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 4.9477e-05 - accuracy: 1.0000 - val_loss: 11.8905 - val_accuracy: 0.2942\n",
            "Epoch 479/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 2.6869e-05 - accuracy: 1.0000 - val_loss: 11.9009 - val_accuracy: 0.2942\n",
            "Epoch 480/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 8.8814e-05 - accuracy: 1.0000 - val_loss: 11.9929 - val_accuracy: 0.2942\n",
            "Epoch 481/500\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 2.6611e-05 - accuracy: 1.0000 - val_loss: 12.0360 - val_accuracy: 0.2942\n",
            "Epoch 482/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 7.0770e-05 - accuracy: 1.0000 - val_loss: 11.9552 - val_accuracy: 0.2942\n",
            "Epoch 483/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 7.0290e-05 - accuracy: 1.0000 - val_loss: 11.8890 - val_accuracy: 0.2942\n",
            "Epoch 484/500\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0043 - accuracy: 0.9992 - val_loss: 11.7902 - val_accuracy: 0.2942\n",
            "Epoch 485/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0027 - accuracy: 0.9996 - val_loss: 11.7733 - val_accuracy: 0.2942\n",
            "Epoch 486/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 2.6234e-04 - accuracy: 1.0000 - val_loss: 11.3825 - val_accuracy: 0.2942\n",
            "Epoch 487/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 1.8862e-04 - accuracy: 1.0000 - val_loss: 11.4327 - val_accuracy: 0.2942\n",
            "Epoch 488/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 2.4090e-05 - accuracy: 1.0000 - val_loss: 11.3966 - val_accuracy: 0.2942\n",
            "Epoch 489/500\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0065 - accuracy: 0.9996 - val_loss: 11.3452 - val_accuracy: 0.2942\n",
            "Epoch 490/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0017 - accuracy: 0.9992 - val_loss: 11.7998 - val_accuracy: 0.2942\n",
            "Epoch 491/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 3.4128e-05 - accuracy: 1.0000 - val_loss: 11.9440 - val_accuracy: 0.2942\n",
            "Epoch 492/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 12.2202 - val_accuracy: 0.2942\n",
            "Epoch 493/500\n",
            "84/84 [==============================] - 1s 7ms/step - loss: 4.6321e-04 - accuracy: 0.9996 - val_loss: 12.6255 - val_accuracy: 0.2942\n",
            "Epoch 494/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 6.7674e-05 - accuracy: 1.0000 - val_loss: 12.5963 - val_accuracy: 0.2942\n",
            "Epoch 495/500\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 12.5848 - val_accuracy: 0.2942\n",
            "Epoch 496/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 5.2764e-04 - accuracy: 1.0000 - val_loss: 13.2872 - val_accuracy: 0.2942\n",
            "Epoch 497/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 2.0810e-04 - accuracy: 1.0000 - val_loss: 13.1999 - val_accuracy: 0.2942\n",
            "Epoch 498/500\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 4.1873e-04 - accuracy: 1.0000 - val_loss: 13.3699 - val_accuracy: 0.2942\n",
            "Epoch 499/500\n",
            "84/84 [==============================] - 1s 8ms/step - loss: 0.0055 - accuracy: 0.9989 - val_loss: 10.8553 - val_accuracy: 0.2942\n",
            "Epoch 500/500\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 0.0066 - accuracy: 0.9989 - val_loss: 10.4604 - val_accuracy: 0.2942\n"
          ]
        }
      ]
    }
  ]
}